# http2

http/2.0在2015年发布，由谷歌主导，在chrome首先实现

为了解决http1.1的问题
1. 线头阻塞的问题 head of line blocking 流水线头部阻塞， http的线头阻塞
    - 浏览器通常保持6个左右的链接
 
2. http标头冗余问题 头部总是有重复内容


http2引入了 数据帧 的处理流程

在应用层把数据拆为多个相应数据帧，每个数据帧包含自己携带的数据并用stream id作为标识。
> 就是把数据铭文消息和http头拆分成二进制数据帧进行传输。 数据流就是把数据切开剁碎形成数据帧，到达目的主机后根据stream id进行组装
> 帮助tcp干了分帧的活
> TODO: 在tcp的时候，建立tcp连接的时候多出4个比特，用于根据mtu协商mss的大小，这里如何处理呢？

这样错，也规避了大文件传输大小的限制。 在应用层就把文件给剁碎了。（这里是绕过iis或者apache等http服务对单个请求有限制），双方以流的形式交互，
客户端以流的方式请求，服务器端以流的方式响应，这样就形成**双向数据流**。
数据帧使用stream id做标识，然后按照stream id将多个数据帧识别为一个**消息**.
http2.0是一个客户端与一个服务端建立一个连接，再加上双向数据流，客户端和服务器的交互可以重用它们建立的单个连接传递数据，这就是**多路复用**。

既然数据流中标注了stream id,那么传输数据流的顺序可以是乱序的，当然也是可以给数据流先手顺序设置优先权重的。

无论服务端还是客户端接收到数据流，先到的在缓冲区先组装，不用等到前一个数据请求完成，再处理下一个，这就解决了http1.1的线头阻塞的问题。

## http/2.0真正解决线头阻塞的问题了吗？

只解决http的线头的阻塞问题。但是http2.0依旧使用tcp的，无法解决tcp的线头阻塞问题。


http/2.0更利于服务器推送，和前端的sse。


## http/2.0标头压缩功能

http1.1总是使用重复的标头，浪费流量，使相应变慢。


[HPACK编码标准资料]( https://httpwg.org/specs/rfc7541.html )
[QPACK编码标准资料](https://www.ietf.org/archive/id/draft-ietf-quic-qpack-21.html#name-static-table-2)


http/2.0使用HPACK来编解码http标头
原理就是建立连接的客户端和服务器双方都维护着61个条目的只读静态表和可动态添加条目的空白动态表。当请求发送的时候先查找表中的名值，名头完全匹配，直接提取该标头在表中对应的整数索引，用于表示该名值对。如果只匹配名称，则使用索引表示名称，值就用霍夫曼的算法进行编码后表示。表中没有的就按照顺序添加到动态表中，在使用霍夫曼算法编码名值对字符后传给服务器。服务器则使用表中的索引解码，再使用霍夫曼算法编码名值对字符后传递给服务器，服务器则反之，使用表中的索引解码，或者按顺序使用霍夫曼算法进行解码后添加到服务器动态表中。这样保证双方的表中保存的数据和整数索引是一致的。在之后的所有请求中的标头会越来越小。

之后的交互减少了网络交互的数据流量，来提高交互的效率。

:path这种标头是伪标头，为了兼容之前的版本。 
http/2.0的标头都是名值对，之前的版本的标头都存在标头行，http2将http1的标头行拆分成名值后的名称前加上了冒号。方便与http2的标头名称进行区分。

## Huffman编码算法，以作者命名最优前缀码算法

TODO: Huffman编码算法


## 了解HPACK原理后是不是可以更好的优化网络请求？

客户端对同一个网站交互越多，动态表积累的条目就会越多，标头的压缩的效果就越好。

所以交互最好是同域名，或者同域下不同二级域名指向的服务器IP是一样的，而且使用的是同一个加密证书。这样确保会使用相同的静动态标头表。
这也解释了为啥泛域名的ssl的加密证书每年会这么贵的原因。

## HPACK是按照顺序编解码标头的，是不是也会造成线头阻塞问题？

是的。 http2数据流可以是无序的，请求到达服务器后，请求携带编码后的标头中，发现有个请求标头索引比服务器编码动态标头表中的最大索引值还大(就是没有),就造成无法解码，而导致使用该请求标头相关的数据流都会被阻塞。直到携带该索引的表示的正确的霍夫曼编码的数据流到达后，才能解码之前请求的数据流。如果这条数据流丢包了，就会造成塞车的时间更长。目前这个问题就只能等到http3去解决了。





## 多路复用

- 它真的就比之前版本的按顺序传输更快吗？
- 它真的不会有其他的线头阻塞问题了吗？


答案是： 不一定。

http2强制了必须使用http2加密的超文本协议传输

但是相比不使用https的http1.1，http2在加密交互的过程中最少增加了两个往返的时间，就是2RRT往返时间，减慢了交互速度。

多路复用中数据被切碎成数据帧，并且使用stream id标识打包成数据流传输，这样可以乱序传输了。

假设一次请求一个js文件，一个css文件，一个分成3份、一个分成5份，
对比下顺序传输和乱序传输，顺序的单个文件比乱序的更快。但是总的到达时间是一致的。

http2的多路复用考虑具体情况来进行优化使用了。

另一个情况，在数据传输的时候少了一帧，就需要至少往返一次服务器，拿丢失的数据帧副本，虽然没有阻塞其它的消息，但是确实是阻塞了自己。这个问题在http3的多路复用也存在这个问题。
虽然这种数据传输之间的线头阻塞是概率问题，但确实存在。











