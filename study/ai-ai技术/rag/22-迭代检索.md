# è¿­ä»£æ£€ç´¢

ä¸€æˆ·è¯æ€»ç»“ï¼š æ ¹æ®ç»“æœç¡®å®šæ˜¯å¦é‡å¤æŸ¥è¯¢ä»¥è¾¾åˆ°å‡†ç¡®çš„ç»“æœ


ç”¨æˆ·è¾“å…¥é—®é¢˜ -> æŸ¥è¯¢é‡æ„ -> æ£€ç´¢ç›¸å…³æ–‡æ¡£ -> æ–‡æ¡£æ’åºå’Œç­›é€‰ -> ä¸Šä¸‹æ–‡æ‹¼æ¥ -> LLMç”Ÿæˆå›ç­” -> è¾“å‡ºç­”æ¡ˆ

- å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥æˆä¸€ä¸ªä¸Šä¸‹æ–‡æ®µè½ã€‚
- ä¸åŸå§‹é—®é¢˜ä¸€èµ·è¾“å…¥åˆ° LLMã€‚

- LLM æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚
- å¦‚æœæ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¯ä»¥æç¤ºç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ï¼Œæˆ–é€šè¿‡è¿­ä»£ç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‚

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# å‡†å¤‡æ–‡æ¡£é›†åˆ
documents = [
    "å¤ªé˜³æ˜¯å¤ªé˜³ç³»ä¸­å¿ƒçš„æ’æ˜Ÿï¼Œå®ƒçš„è´¨é‡çº¦ä¸ºæœ¨æ˜Ÿçš„1000å€ã€‚",
    "æ°´æ˜Ÿæ˜¯å¤ªé˜³ç³»æœ€å†…å±‚ä¹Ÿæ˜¯æœ€å°çš„è¡Œæ˜Ÿï¼Œè¡¨é¢æ¸©åº¦å˜åŒ–å¾ˆå¤§ã€‚",
    "é‡‘æ˜Ÿæ˜¯å¤ªé˜³ç³»ä¸­ç¬¬äºŒé¢—è¡Œæ˜Ÿï¼Œè¢«ç§°ä¸ºåœ°çƒçš„å§å¦¹æ˜Ÿã€‚",
    "åœ°çƒæ˜¯å¤ªé˜³ç³»ä¸­ç¬¬ä¸‰é¢—è¡Œæ˜Ÿï¼Œæ˜¯ç›®å‰å·²çŸ¥å”¯ä¸€å­•è‚²ç”Ÿå‘½çš„æ˜Ÿçƒã€‚",
    "ç«æ˜Ÿæ˜¯å¤ªé˜³ç³»ä¸­ç¬¬å››é¢—è¡Œæ˜Ÿï¼Œè¡¨é¢æœ‰çº¢è‰²æ°§åŒ–é“ï¼Œè¢«ç§°ä¸ºçº¢è‰²æ˜Ÿçƒã€‚",
    "æœ¨æ˜Ÿæ˜¯å¤ªé˜³ç³»æœ€å¤§çš„è¡Œæ˜Ÿï¼Œå®ƒçš„å¤§çº¢æ–‘æ˜¯ä¸€ä¸ªæŒç»­äº†è‡³å°‘400å¹´çš„é£æš´ã€‚",
    "åœŸæ˜Ÿä»¥å…¶æ˜¾è‘—çš„ç¯ç³»ç»Ÿé—»åï¼Œæ˜¯å¤ªé˜³ç³»ä¸­ç¬¬äºŒå¤§çš„è¡Œæ˜Ÿã€‚",
    "å¤©ç‹æ˜Ÿå’Œæµ·ç‹æ˜Ÿæ˜¯å¤ªé˜³ç³»çš„å†°å·¨æ˜Ÿï¼Œå®ƒä»¬çš„å¤§æ°”ä¸»è¦ç”±æ°¢ã€æ°¦å’Œç”²çƒ·ç»„æˆã€‚"
]

# åˆå§‹åŒ–å¿…è¦ç»„ä»¶
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)
docs = splitter.create_documents(documents)
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(docs, embeddings)
llm = ChatOpenAI(temperature=0.3, model_name="gpt-4")

def create_iterative_chain():
    """åˆ›å»ºè¿­ä»£æ£€ç´¢é“¾"""
    # åˆ›å»ºè¯„ä¼°æç¤ºæ¨¡æ¿
    evaluation_prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œç”Ÿæˆæ–°çš„æŸ¥è¯¢æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚

<context>
{context}
</context>

å½“å‰é—®é¢˜: {input}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
1. å¦‚æœå½“å‰ä¿¡æ¯è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œä»¥"SUFFICIENT:"å¼€å¤´ï¼Œç„¶åç»™å‡ºå®Œæ•´ç­”æ¡ˆ
2. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œä»¥"QUERY:"å¼€å¤´ï¼Œç„¶åç»™å‡ºæ–°çš„æŸ¥è¯¢

æ³¨æ„ï¼š
- æ–°æŸ¥è¯¢åº”è¯¥é’ˆå¯¹æ€§åœ°è·å–ç¼ºå¤±çš„å…³é”®ä¿¡æ¯
- ç¡®ä¿æŸ¥è¯¢ä¸åŸå§‹é—®é¢˜ç›¸å…³
- è€ƒè™‘å·²æœ‰ä¿¡æ¯ï¼Œé¿å…é‡å¤æŸ¥è¯¢
""")
    
    # åˆ›å»ºæ–‡æ¡£é“¾
    document_chain = create_stuff_documents_chain(llm, evaluation_prompt)
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vector_store.as_retriever(search_kwargs={"k": 2})
    
    # åˆ›å»ºæ£€ç´¢é“¾
    return create_retrieval_chain(retriever, document_chain)

def enhanced_iterative_retrieval(query, max_rounds=3):
    """
    å¢å¼ºç‰ˆè¿­ä»£æ£€ç´¢å‡½æ•°
    :param query: åˆå§‹æŸ¥è¯¢
    :param max_rounds: æœ€å¤§è¿­ä»£è½®æ¬¡
    :return: (æœ€ç»ˆç­”æ¡ˆ, æ£€ç´¢å†å²)
    """
    chain = create_iterative_chain()
    current_query = query
    retrieval_history = []
    
    print(f"ğŸ“ åˆå§‹é—®é¢˜: {query}")
    
    for round_num in range(max_rounds):
        print(f"\nğŸ”„ ç¬¬ {round_num + 1} è½®æ£€ç´¢:")
        print(f"å½“å‰æŸ¥è¯¢: {current_query}")
        
        # æ‰§è¡Œæ£€ç´¢
        response = chain.invoke({"input": current_query})
        answer = response["answer"]
        
        # è®°å½•æœ¬è½®æ£€ç´¢
        retrieval_history.append({
            "round": round_num + 1,
            "query": current_query,
            "response": answer
        })
        
        print(f"LLMå“åº”: {answer}")
        
        # æ£€æŸ¥æ˜¯å¦å·²è·å¾—è¶³å¤Ÿä¿¡æ¯
        if answer.startswith("SUFFICIENT:"):
            return answer[11:].strip(), retrieval_history
        
        # æå–æ–°çš„æŸ¥è¯¢
        if answer.startswith("QUERY:"):
            current_query = answer[6:].strip()
        else:
            return "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡¥å……æŸ¥è¯¢", retrieval_history
    
    return "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶", retrieval_history

def run_example_query(query):
    """è¿è¡Œç¤ºä¾‹æŸ¥è¯¢å¹¶å±•ç¤ºç»“æœ"""
    print(f"\n{'='*50}")
    print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
    
    answer, history = enhanced_iterative_retrieval(query)
    
    print(f"\nğŸ“Š æ£€ç´¢è¿‡ç¨‹æ‘˜è¦:")
    for round_data in history:
        print(f"\nè½®æ¬¡ {round_data['round']}:")
        print(f"æŸ¥è¯¢: {round_data['query']}")
        print(f"å“åº”: {round_data['response']}")
    
    print(f"\nâœ¨ æœ€ç»ˆç­”æ¡ˆ: {answer}")
    print(f"{'='*50}\n")

# è¿è¡Œç¤ºä¾‹æŸ¥è¯¢
example_queries = [
    "æœ¨æ˜Ÿæœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿ",
    "å¤ªé˜³ç³»ä¸­æœ€å¤§çš„ä¸¤ä¸ªè¡Œæ˜Ÿæ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
    "å“ªäº›è¡Œæ˜Ÿè¢«ç§°ä¸ºå†°å·¨æ˜Ÿï¼Œå®ƒä»¬çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
]

for query in example_queries:
    run_example_query(query)
``` 

- [å®˜æ–¹ä¾‹å­](https://python.langchain.com/docs/integrations/vectorstores/aperturedb/)