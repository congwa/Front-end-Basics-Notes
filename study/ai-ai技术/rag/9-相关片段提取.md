# 相关片段提取 (RSE)

## 1. 概述

相关片段提取(RSE)是一种在RAG系统中优化检索结果的方法，它通过重建连续文本的多块片段来提供更好的上下文。

相关片段提取 (RSE) 是一种从检索到的块中重建连续文本的多块片段的方法。此步骤发生在矢量搜索（以及可选的重新排名）之后，但在将检索到的上下文呈现给LLM之前。此方法确保附近的块按照它们在原始文档中出现的顺序呈现给LLM 。它还添加了未标记为相关的块，但夹在高度相关的块之间，进一步改善了提供给LLM上下文。此方法显着提高了 RAG 性能，如本笔记本末尾提供的评估结果所示。

### 主要特点
- 在向量搜索和重排序之后执行
- 保持文档块的原始顺序
- 包含被相关块"夹住"的中间块
- 显著提升RAG性能

## 2. 工作流程示例

### 2.1 初始检索结果
```text
文档块1 (相似度: 0.85, 排名: 1)
位置: 第3块
内容: "企业计划在2025年前将碳排放..."

文档块2 (相似度: 0.82, 排名: 2)
位置: 第5块
内容: "通过技术创新降低能源消耗..."
```

### 2.2 RSE处理后
```text
完整片段:
第3块: "企业计划在2025年前将碳排放..."
第4块: "具体措施包括更新设备和优化工艺..."（原本未被检索，但被包含）
第5块: "通过技术创新降低能源消耗..."
```

## 3. 核心算法

### 3.1 块值计算
```python
def calculate_chunk_value(similarity_score: float, rank: int, threshold: float) -> float:
    # 结合相似度和排名
    combined_score = (similarity_score + (1 / (rank + 1))) / 2
    # 减去阈值
    return combined_score - threshold
```

### 3.2 示例计算
```text
原始块值: [-0.2, -0.2, 0.4, 0.8, -0.1]
片段1(块2-3): 0.4 + 0.8 = 1.2
片段2(块3-4): 0.8 + (-0.1) = 0.7
```

## 4. 实现考虑因素

### 4.1 数据存储要求
- 需要快速访问的键值存储
- 使用doc_id和chunk_index作为键
- 存储完整的块内容

### 4.2 优化策略
1. **分块策略**
   - 无重叠分块
   - 保持块的连续性
   - 适当的块大小选择

2. **相关性计算**
   - 结合相似度分数
   - 考虑排名影响
   - 设置合适的阈值

3. **片段构建**
   - 最大片段大小限制
   - 片段价值评估
   - 效率优化

## 5. 性能优势

1. **动态适应**
   - 可以处理不同长度的查询需求
   - 灵活平衡精确性和上下文

2. **上下文完整性**
   - 保持文档的连贯性
   - 提供更完整的背景信息

3. **检索质量**
   - 提高相关信息的召回率
   - 减少信息碎片化

## 6. 应用示例

```python
# RSE处理流程示例
def process_rse(retrieved_chunks: List[Chunk], threshold: float = 0.2) -> List[Chunk]:
    # 1. 计算块值
    chunk_values = []
    for chunk in retrieved_chunks:
        value = calculate_chunk_value(
            chunk.similarity_score,
            chunk.rank,
            threshold
        )
        chunk_values.append(value)
    
    # 2. 寻找最优片段
    best_segment = find_best_segment(chunk_values)
    
    # 3. 重建完整片段
    return reconstruct_segment(retrieved_chunks, best_segment)
```

## 7. 最佳实践

1. **参数调优**
   - 根据具体应用场景调整阈值
   - 优化片段大小限制
   - 平衡效率和质量

2. **集成建议**
   - 与现有RAG系统无缝集成
   - 保持处理流程的模块化
   - 提供灵活的配置选项

3. **性能监控**
   - 跟踪处理时间
   - 评估结果质量
   - 持续优化改进


## 代码示例

```py
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import numpy as np
from collections import defaultdict
import redis
import faiss
from langchain_text_splitters import RecursiveCharacterTextSplitter
import torch
from transformers import AutoTokenizer, AutoModel
import os

@dataclass
class Chunk:
    """文档块"""
    doc_id: str
    chunk_index: int
    content: str
    embedding: np.ndarray = None
    similarity_score: float = 0.0
    rank: int = 0
    chunk_value: float = 0.0
    size: int = 0

class ChunkValueCalculator:
    """块值计算器"""
    def __init__(self, threshold: float = 0.2):
        self.threshold = threshold
    
    def calculate_chunk_value(self, chunk: Chunk) -> float:
        """计算块的价值
        结合相似度分数和排名，并应用阈值
        """
        # 结合相似度和排名
        rank_score = 1 / (chunk.rank + 1)
        combined_score = (chunk.similarity_score + rank_score) / 2
        
        # 应用阈值
        chunk_value = combined_score - self.threshold
        return chunk_value

class DynamicChunkStore:
    """动态块存储系统"""
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.kv_store = redis.Redis(host=redis_host, port=redis_port)
        self.vector_dim = 768
        self.vector_store = faiss.IndexFlatL2(self.vector_dim)
        self.chunk_mapping = defaultdict(dict)
    
    def store_chunk(self, chunk: Chunk):
        """存储文档块"""
        key = f"{chunk.doc_id}:{chunk.chunk_index}"
        self.kv_store.hset(key, mapping={
            'content': chunk.content,
            'size': chunk.size,
            'embedding': chunk.embedding.tobytes() if chunk.embedding is not None else b'',
            'similarity_score': str(chunk.similarity_score),
            'rank': str(chunk.rank),
            'chunk_value': str(chunk.chunk_value)
        })
        
        if chunk.embedding is not None:
            index = len(self.chunk_mapping)
            self.vector_store.add(np.array([chunk.embedding]))
            self.chunk_mapping[chunk.doc_id][chunk.chunk_index] = index

    def get_chunk(self, doc_id: str, chunk_index: int) -> Optional[Chunk]:
        """快速检索指定块"""
        key = f"{doc_id}:{chunk_index}"
        data = self.kv_store.hgetall(key)
        if not data:
            return None
        
        chunk = Chunk(
            doc_id=doc_id,
            chunk_index=chunk_index,
            content=data[b'content'].decode(),
            size=int(data[b'size']),
            similarity_score=float(data[b'similarity_score']),
            rank=int(data[b'rank']),
            chunk_value=float(data[b'chunk_value'])
        )
        if b'embedding' in data and data[b'embedding']:
            chunk.embedding = np.frombuffer(data[b'embedding'], dtype=np.float32)
        return chunk

class MaxSumSegmentFinder:
    """最大和子数组片段查找器"""
    def __init__(self, max_segment_size: int = 5):
        self.max_segment_size = max_segment_size
    
    def find_best_segments(self, chunks: List[Chunk]) -> List[List[Chunk]]:
        """查找最优片段集合"""
        chunk_values = [chunk.chunk_value for chunk in chunks]
        segments = []
        
        # 使用滑动窗口查找最优片段
        n = len(chunks)
        for start in range(n):
            current_sum = 0
            for length in range(1, min(self.max_segment_size + 1, n - start + 1)):
                current_sum += chunk_values[start + length - 1]
                
                # 如果片段值为正，添加到结果中
                if current_sum > 0:
                    segment = chunks[start:start + length]
                    segments.append(segment)
        
        return self._merge_overlapping_segments(segments)
    
    def _merge_overlapping_segments(self, segments: List[List[Chunk]]) -> List[List[Chunk]]:
        """合并重叠片段"""
        if not segments:
            return []
        
        segments.sort(key=lambda x: (x[0].doc_id, x[0].chunk_index))
        merged = []
        current = segments[0]
        
        for segment in segments[1:]:
            if (segment[0].doc_id == current[-1].doc_id and 
                segment[0].chunk_index <= current[-1].chunk_index + 1):
                # 合并重叠片段
                current.extend([c for c in segment if c.chunk_index > current[-1].chunk_index])
            else:
                merged.append(current)
                current = segment
        
        merged.append(current)
        return merged

class RSEProcessor:
    """RSE处理器"""
    def __init__(self, 
                 chunk_store: DynamicChunkStore,
                 value_calculator: ChunkValueCalculator,
                 segment_finder: MaxSumSegmentFinder):
        self.chunk_store = chunk_store
        self.value_calculator = value_calculator
        self.segment_finder = segment_finder
    
    def process_chunks(self, retrieved_chunks: List[Chunk]) -> List[List[Chunk]]:
        """处理检索到的块并构建最优片段"""
        # 1. 计算块值
        for chunk in retrieved_chunks:
            chunk.chunk_value = self.value_calculator.calculate_chunk_value(chunk)
        
        # 2. 查找最优片段
        initial_segments = self.segment_finder.find_best_segments(retrieved_chunks)
        
        # 3. 补充中间块
        complete_segments = self._complete_segments(initial_segments)
        
        return complete_segments
    
    def _complete_segments(self, segments: List[List[Chunk]]) -> List[List[Chunk]]:
        """补充片段中的缺失块"""
        completed = []
        for segment in segments:
            if not segment:
                continue
                
            complete_segment = []
            doc_id = segment[0].doc_id
            start_idx = segment[0].chunk_index
            end_idx = segment[-1].chunk_index
            
            for idx in range(start_idx, end_idx + 1):
                chunk = next(
                    (c for c in segment if c.chunk_index == idx),
                    self.chunk_store.get_chunk(doc_id, idx)
                )
                if chunk:
                    complete_segment.append(chunk)
            
            if complete_segment:
                completed.append(complete_segment)
        
        return completed

class DocumentProcessor:
    """文档处理器"""
    def __init__(self, 
                 chunk_size: int = 800,
                 model_name: str = "BAAI/bge-large-zh"):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=0,
            length_function=len
        )
        # 初始化embedding模型
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        
    def process_document(self, 
                        doc_id: str, 
                        content: str, 
                        chunk_store: DynamicChunkStore) -> List[Chunk]:
        """处理文档并存储块"""
        # 1. 分块
        chunks = self.text_splitter.create_documents([content])
        
        # 2. 创建块对象并计算embedding
        doc_chunks = []
        for i, chunk in enumerate(chunks):
            embedding = self._get_embedding(chunk.page_content)
            doc_chunk = Chunk(
                doc_id=doc_id,
                chunk_index=i,
                content=chunk.page_content,
                embedding=embedding,
                size=len(chunk.page_content)
            )
            doc_chunks.append(doc_chunk)
            # 存储块
            chunk_store.store_chunk(doc_chunk)
        
        return doc_chunks
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本的embedding"""
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()
        return embeddings[0]

class QueryProcessor:
    """查询处理器"""
    def __init__(self, 
                 chunk_store: DynamicChunkStore,
                 rse_processor: RSEProcessor,
                 top_k: int = 5):
        self.chunk_store = chunk_store
        self.rse_processor = rse_processor
        self.top_k = top_k
        
    def process_query(self, query: str, doc_processor: DocumentProcessor) -> List[List[Chunk]]:
        """处理用户查询"""
        # 1. 获取查询的embedding
        query_embedding = doc_processor._get_embedding(query)
        
        # 2. 向量检索
        D, I = self.chunk_store.vector_store.search(
            np.array([query_embedding]), 
            self.top_k
        )
        
        # 3. 获取检索到的块
        retrieved_chunks = []
        for i, (dist, idx) in enumerate(zip(D[0], I[0])):
            for doc_id, chunk_dict in self.chunk_store.chunk_mapping.items():
                for chunk_index, vector_index in chunk_dict.items():
                    if vector_index == idx:
                        chunk = self.chunk_store.get_chunk(doc_id, chunk_index)
                        if chunk:
                            chunk.similarity_score = 1 - dist  # 转换距离为相似度
                            chunk.rank = i
                            retrieved_chunks.append(chunk)
        
        # 4. RSE处理
        segments = self.rse_processor.process_chunks(retrieved_chunks)
        return segments

def main():
    # 初始化组件
    chunk_store = DynamicChunkStore()
    value_calculator = ChunkValueCalculator(threshold=0.2)
    segment_finder = MaxSumSegmentFinder(max_segment_size=5)
    rse_processor = RSEProcessor(chunk_store, value_calculator, segment_finder)
    doc_processor = DocumentProcessor()
    query_processor = QueryProcessor(chunk_store, rse_processor)
    
    # 示例：处理文档
    documents = {
        "doc1": """人工智能在医疗领域的应用正在快速发展。AI辅助诊断系统可以分析医学影像,
                帮助医生更准确地发现疾病。此外,AI还可以通过分析大量病历数据,为医生提供治疗
                方案建议,提高诊疗效率。在药物研发方面,AI也发挥着重要作用...""",
        "doc2": """近年来,自动驾驶技术取得重大突破。多家科技公司和汽车制造商都在积极开发
                自动驾驶汽车。这项技术通过计算机视觉、深度学习等AI技术,可以实现车辆的自动
                控制。目前已经在多个城市开展了路测..."""
    }
    
    print("开始处理文档...")
    for doc_id, content in documents.items():
        chunks = doc_processor.process_document(doc_id, content, chunk_store)
        print(f"文档 {doc_id} 已处理，生成了 {len(chunks)} 个块")
    
    # 示例：处理查询
    query = "AI技术在医疗诊断中有哪些具体应用?"
    print(f"\n处理查询: {query}")
    
    segments = query_processor.process_query(query, doc_processor)
    
    # 打印结果
    print("\n查询结果:")
    for i, segment in enumerate(segments):
        print(f"\n片段 {i+1}:")
        print(f"文档: {segment[0].doc_id}")
        print(f"范围: 块 {segment[0].chunk_index} 到 块 {segment[-1].chunk_index}")
        print(f"相似度: {[f'{c.similarity_score:.2f}' for c in segment]}")
        print("内容:")
        for chunk in segment:
            print(f"- {chunk.content[:100]}...")

if __name__ == "__main__":
    main()

```

