# 上下文内容丰富 -  上下文压缩

一句话总结： **智能压缩提取关键信息**,在上下文压缩中比较通用

## 压缩时机

压缩的时机

1. 检索后生成答案之前
   - 在从文档中检索到相关内容后，将其压缩以保留关键信息，从而减少上下文大小，便于后续生成答案。
2. 处理大文本块时
   - 当文档或检索到的片段过大，超过 LLM 的上下文窗口限制时，需要先进行压缩以适配模型处理能力。
3. 多轮问答中
   - 在连续问答中，对历史检索内容或上下文进行压缩，以便保持会话的流畅性和上下文的相关性。
4. 存储和归档前
   - 在将文档存入矢量数据库或归档存储时，可以先压缩内容，降低存储空间，同时保留信息价值。
5. 性能优化时
   - 为减少不必要的计算开销和数据传输量，在处理文档的早期阶段或流式传输内容时进行压缩。


压缩检索到的信息，同时保留与查询相关的内容。

使用 LLM 压缩或汇总检索到的块，保留与查询相关的关键信息

文档检索中的上下文压缩提供了一种增强信息检索系统的质量和效率的强大方法。通过智能地提取和压缩相关信息，它可以为查询提供更有针对性和上下文感知的响应

### [compress_documents 方法](https://python.langchain.com/api_reference/_modules/langchain_core/documents/compressor.html#BaseDocumentCompressor.compress_documents)
根据给定查询上下文压缩输入文档。

参数说明:
- documents: 输入的文档序列(类型为 Sequence[Document])
- query: 用于指导压缩的查询上下文
- callbacks: 可选的回调函数,用于记录或扩展操作

返回值:
- 压缩后的文档序列

注意:
- 抽象方法,继承类必须实现具体逻辑

### acompress_documents 方法
提供文档压缩的异步版本。

实现细节:
- 默认实现基于 run_in_executor,在后台线程运行 compress_documents 方法
- 参数和返回值与 compress_documents 相同
- 允许开发者覆盖该方法以实现自定义异步逻辑

```py
# 创建检索器
retriever = vector_store.as_retriever()

# 创建上下文压缩器
# 使用温度为0的GPT-4模型，最大token数为4000
llm = ChatOpenAI(temperature=0, model_name="gpt-4o-mini", max_tokens=4000)
compressor = LLMChainExtractor.from_llm(llm)

# 将检索器与压缩器组合
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,  # 基础压缩器
    base_retriever=retriever     # 基础检索器
)

# 使用压缩检索器创建问答链
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,                           # 语言模型
    retriever=compression_retriever,    # 压缩检索器
    return_source_documents=True        # 返回源文档
)
```
## 总结

### Chain-of-Thought

### 压缩器的本质

- 基于规则的压缩：使用人工定义的规则提取关键内容。
- 基于模型的压缩：利用机器学习或深度学习模型提取或生成简化内容。
- 混合方法：结合规则和模型，利用各自的优势


### 压缩器的作用 - 压缩器通过智能过滤或重组内容，使得输入模型的数据更加紧凑且相关

1. 选一个大块文本进行压缩
2. 提示词上下文压缩
3. 检索到的相关内容压缩


4. **上下文相关内容提取**  
   压缩器的主要作用是根据查询（`query`）从检索到的文档中提取出最相关的内容，并过滤掉与问题无关的部分。这种方式保证了输入到语言模型中的信息是与问题高度相关的。

   **具体操作**：
   - 从检索器（`retriever`）返回的文档中提取内容。
   - 使用语言模型生成精简的上下文，以降低冗余。

5. **减少冗余，优化 token 使用**  
   在许多实际应用中，单个文档的内容可能过长，无法一次性传递给模型。压缩器通过过滤无关内容和压缩关键信息，减少了模型的输入大小，使其更符合语言模型的 token 限制。

6. **提高问答准确性**  
   - 通过去除噪声信息，压缩器为模型提供了更加精准的上下文，提升了生成答案的准确性。
   - 避免因冗余信息或不相关内容导致的答案偏差。

7. **改善性能与效率**  
   - 文档越小，处理速度越快。通过压缩器，整个问答流程更加高效。
   - 节省计算资源，尤其是在调用大型语言模型时。

---

### 压缩器的实现逻辑

1. **初始化**  
   - 使用 `LLMChainExtractor.from_llm(self.llm)` 初始化压缩器，其中 `self.llm` 是指定的语言模型。
   - `LLMChainExtractor` 是一个基于链式语言模型的方法，它能够根据给定的查询提取文档的相关部分。

2. **与检索器组合**  
   - `ContextualCompressionRetriever` 将压缩器（`base_compressor`）和检索器（`base_retriever`）组合在一起：
     - 检索器负责从向量存储中找到可能相关的文档。
     - 压缩器对这些文档进行进一步处理，提取最相关的内容。

3. **运行流程**  
   - 用户提供查询后，检索器返回一组候选文档。
   - 压缩器根据查询分析候选文档并提取关键信息。
   - 生成的精简文档被传递给问答链（`RetrievalQA`），从而生成答案。

---

### 举例说明

假设系统检索到以下两个文档：
- **文档 A**：包含 10 页与气候变化相关的详细数据。
- **文档 B**：包含部分气候变化讨论及其对农业的影响。

用户的查询是：*“气候变化对农业的主要影响是什么？”*

1. **检索器的作用**：
   - 检索到文档 A 和 B。

2. **压缩器的作用**：
   - 提取文档中仅与农业影响相关的段落，生成一个简化版本：
     ```
     文档 A：气候变化导致干旱增加，粮食产量下降。
     文档 B：温度升高改变了农业周期。
     ```

3. **语言模型的作用**：
   - 基于压缩后的内容生成精准答案。

---

### 压缩器的优势

1. **高效处理长文档**：直接过滤无关内容，减少传输和计算负担。
2. **提升模型的上下文理解**：提供清晰的上下文，避免模型在无关内容中迷失。
3. **灵活性强**：可以根据查询动态调整压缩策略，适配不同场景。


- [langchain 上下文压缩](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/contextual_compression.py)
- [langchain 上下文压缩文档](https://python.langchain.com/docs/how_to/contextual_compression/#adding-contextual-compression-with-an-llmchainextractor)