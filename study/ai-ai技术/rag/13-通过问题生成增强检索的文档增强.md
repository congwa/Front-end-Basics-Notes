# 上下文内容丰富 - 通过问题生成的文档增强

一句话总结：**通过使用 LLM 生成问题并将其附加到原始片段，增强文档结构以优化检索效果**

1. **文档处理**：  
   - 通过正则表达式将文档分割为固定大小的片段。
   - 支持文档级别和片段级别的分割，以适应不同的场景需求。

2. **问题生成**：  
   - 使用 LLM（如 OpenAI 的 GPT 模型）对每个片段生成可能的问题列表。
   - 通过清洗和去重，确保问题的质量和唯一性。

3. **数据增强**：  
   - 将生成的问题附加到原始片段中，形成增强后的文档结构。
   - 生成问题的数量和类型取决于用户的参数设置（如每个文档生成的问题数量）。

4. **向量化与存储**：  
   - 使用 OpenAI 的嵌入模型对增强后的文档和问题生成向量。
   - 将向量存储在 FAISS 数据库中，为高效检索提供支持。

5. **检索与问答生成**：  
   - 基于用户查询，通过 FAISS 检索最相关的增强文档或问题片段。
   - 使用检索到的片段作为上下文，结合 LLM 提供精确回答。


- **检索增强**：通过问题生成扩展了原始文档的语义范围，增加了检索的可能性。
- **上下文优化**：生成的多样化问题有助于 LLM 理解用户查询意图，更好地匹配上下文。
- **灵活性**：文档和片段级别的增强方式适应了不同场景的需求。

```py
import sys
import os
import re
from langchain.docstore.document import Document
from langchain.vectorstores import FAISS
from enum import Enum
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from typing import Any, Dict, List, Tuple
from pydantic import BaseModel, Field
import argparse

from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# 设置 OpenAI API 的密钥
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

# 将父目录添加到路径中
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))

from helper_functions import *


class QuestionGeneration(Enum):
    """
    枚举类，用于指定文档处理中的问题生成级别。
    """
    DOCUMENT_LEVEL = 1  # 文档级别
    FRAGMENT_LEVEL = 2  # 片段级别


# 文档和片段的配置参数
DOCUMENT_MAX_TOKENS = 4000
DOCUMENT_OVERLAP_TOKENS = 100
FRAGMENT_MAX_TOKENS = 128
FRAGMENT_OVERLAP_TOKENS = 16
QUESTION_GENERATION = QuestionGeneration.DOCUMENT_LEVEL
QUESTIONS_PER_DOCUMENT = 40


class QuestionList(BaseModel):
    question_list: List[str] = Field(..., title="为文档或片段生成的问题列表")


class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):
    """
    OpenAI 嵌入的包装类，提供与 OllamaEmbeddings 类似的接口。
    """
    def __call__(self, query: str) -> List[float]:
        return self.embed_query(query)


def clean_and_filter_questions(questions: List[str]) -> List[str]:
    """
    清洗并过滤生成的问题列表。
    """
    cleaned_questions = []
    for question in questions:
        # 去掉编号和多余空格
        cleaned_question = re.sub(r'^\d+\.\s*', '', question.strip())
        if cleaned_question.endswith('?'):  # 确保是以问号结尾的问题
            cleaned_questions.append(cleaned_question)
    return cleaned_questions


def generate_questions(text: str) -> List[str]:
    """
    使用 LLM 生成与文本相关的问题。
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = PromptTemplate(
        input_variables=["context", "num_questions"],
        template="根据以下上下文：{context}\n\n生成至少 {num_questions} 个与上下文相关的问题列表。"
    )
    chain = prompt | llm.with_structured_output(QuestionList)
    input_data = {"context": text, "num_questions": QUESTIONS_PER_DOCUMENT}
    result = chain.invoke(input_data)
    questions = result.question_list
    return list(set(clean_and_filter_questions(questions)))  # 去重和清洗


def generate_answer(content: str, question: str) -> str:
    """
    使用 LLM 为给定问题和上下文生成回答。
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="根据以下上下文：{context}\n\n提供以下问题的简洁且精确的答案：{question}"
    )
    chain = prompt | llm
    input_data = {"context": content, "question": question}
    return chain.invoke(input_data)


def split_document(document: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """
    将文档分割成多个片段。
    """
    tokens = re.findall(r'\b\w+\b', document)
    chunks = []
    for i in range(0, len(tokens), chunk_size - chunk_overlap):
        chunk_tokens = tokens[i:i + chunk_size]
        chunks.append(chunk_tokens)
        if i + chunk_size >= len(tokens):
            break
    return [" ".join(chunk) for chunk in chunks]


def print_document(comment: str, document: Any) -> None:
    """
    打印文档信息。
    """
    print(f'{comment} (类型: {document.metadata["type"]}, 索引: {document.metadata["index"]}): {document.page_content}')


class DocumentProcessor:
    def __init__(self, content: str, embedding_model: OpenAIEmbeddings):
        self.content = content
        self.embedding_model = embedding_model

    def run(self):
        """
        主处理逻辑：分割文档、生成问题、创建向量数据库。
        """
        text_documents = split_document(self.content, DOCUMENT_MAX_TOKENS, DOCUMENT_OVERLAP_TOKENS)
        print(f'文本文档被分割为: {len(text_documents)} 份')

        documents = []
        counter = 0
        for i, text_document in enumerate(text_documents):
            text_fragments = split_document(text_document, FRAGMENT_MAX_TOKENS, FRAGMENT_OVERLAP_TOKENS)
            print(f'文档 {i} 被分割为: {len(text_fragments)} 个片段')

            for j, text_fragment in enumerate(text_fragments):
                documents.append(Document(
                    page_content=text_fragment,
                    metadata={"type": "原始", "index": counter, "text": text_document}
                ))
                counter += 1

                if QUESTION_GENERATION == QuestionGeneration.FRAGMENT_LEVEL:
                    questions = generate_questions(text_fragment)
                    documents.extend([
                        Document(page_content=question,
                                 metadata={"type": "增强", "index": counter + idx, "text": text_document})
                        for idx, question in enumerate(questions)
                    ])
                    counter += len(questions)
                    print(f'文档 {i} 片段 {j} 生成了: {len(questions)} 个问题')

            if QUESTION_GENERATION == QuestionGeneration.DOCUMENT_LEVEL:
                questions = generate_questions(text_document)
                documents.extend([
                    Document(page_content=question,
                             metadata={"type": "增强", "index": counter + idx, "text": text_document})
                    for idx, question in enumerate(questions)
                ])
                counter += len(questions)
                print(f'文档 {i} 生成了: {len(questions)} 个问题')

        for document in documents:
            print_document("数据集", document)

        print(f'创建存储，计算 {len(documents)} 个 FAISS 文档的嵌入')
        vectorstore = FAISS.from_documents(documents, self.embedding_model)

        print("创建检索器，返回最相关的 FAISS 文档")
        return vectorstore.as_retriever(search_kwargs={"k": 1})


def parse_args():
    """
    解析命令行参数。
    """
    parser = argparse.ArgumentParser(description="处理文档并创建检索器。")
    parser.add_argument('--path', type=str, default='../data/Understanding_Climate_Change.pdf',
                        help="待处理 PDF 文档的路径")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # 加载 PDF 文档内容到字符串变量中
    content = read_pdf_to_string(args.path)

    # 实例化 OpenAI 嵌入模型
    embedding_model = OpenAIEmbeddings()

    # 处理文档并创建检索器
    processor = DocumentProcessor(content, embedding_model)
    document_query_retriever = processor.run()

    # 检索示例
    query = "什么是气候变化？"
    retrieved_docs = document_query_retriever.get_relevant_documents(query)
    print(f"\n查询: {query}")
    print(f"检索到的文档: {retrieved_docs[0].page_content}")

    # 进一步查询示例
    query = "淡水生态系统如何因气候因素的变化而改变？"
    retrieved_documents = document_query_retriever.get_relevant_documents(query)
    for doc in retrieved_documents:
        print_document("检索到的相关片段", doc)

    context = doc.metadata['text']
    answer = generate_answer(context, query)
    print(f'{os.linesep}回答:{os.linesep}{answer}')
```