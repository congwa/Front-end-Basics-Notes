# 上下文内容丰富 - 文档处理的语义分块

## 概述
该代码实现了一种语义分块方法，用于处理和检索 PDF 文档中的信息，该方法首先由 Greg Kamradt 提出，随后在 LangChain 中实现。与基于固定字符或字数分割文本的传统方法不同，语义分块旨在创建更有意义和上下文感知的文本片段。

通过尝试保持文本片段内的语义一致性，它有可能提高检索信息的质量并增强下游 NLP 任务的性能。

## 动机
传统的文本分割方法经常在任意点破坏文档，可能会破坏信息和上下文的流动。语义分块通过尝试在更自然的断点处分割文本来解决此问题，从而保持每个块内的语义一致性。

## 关键步骤
- PDF处理和文本提取
- 使用 LangChain 的 SemanticChunker 进行语义分块
- 使用 FAISS 和 OpenAI 嵌入创建矢量存储
- 用于查询已处理文档的检索器设置

## 方法详情

### 文档预处理
使用自定义`read_pdf_to_string`函数读取 PDF 并将其转换为字符串。

### 语义分块
利用 LangChain 的SemanticChunker和 OpenAI 嵌入。

可以使用三种断点类型：
- 'percentile'：以大于 X 百分位数的差异进行分割。
- 'standard_deviation'：以大于 X 标准差的差异进行分割。
- 'interquartile'：使用四分位距离来确定分割点。

在此实现中，使用"百分位数"方法，阈值为 90。

### 矢量存储创建
- OpenAI 嵌入用于创建语义块的向量表示。
- 根据这些嵌入创建 FAISS 矢量存储，以实现高效的相似性搜索。

### 检索器设置
检索器配置为获取给定查询的前 2 个最相关的块。

## 主要特征
- 上下文感知拆分：尝试保持块内的语义一致性。
- 灵活的配置：允许不同的断点类型和阈值。
- 与高级 NLP 工具集成：使用 OpenAI 嵌入进行分块和检索。

## 这种方法的好处
- 提高连贯性：块更有可能包含完整的想法或想法。
- 更好的检索相关性：通过保留上下文，可以提高检索准确性。
- 适应性：分块方法可以根据文档的性质和检索需求进行调整。
- 更好理解的潜力：LLMs或下游任务可能会通过更连贯的文本片段表现得更好。

## 实施细节
- 使用 OpenAI 的嵌入进行语义分块过程和最终向量表示。
- 采用 FAISS 创建高效的可搜索块索引。
- 检索器设置为返回前 2 个最相关的块，可以根据需要进行调整。


```py
import os
import sys
from dotenv import load_dotenv

# 添加父目录到路径中，因为我们使用notebooks工作
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))
from helper_functions import *
from evaluation.evalute_rag import *

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

# 从.env文件加载环境变量
load_dotenv()

# 设置OpenAI API密钥环境变量
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

# 文档路径
path = "../data/Understanding_Climate_Change.pdf"

# 读取文档内容
content = read_pdf_to_string(path)

# 断点类型：
# 'percentile'：计算句子之间的所有差异，然后将任何大于X百分位数的差异进行分割
# 'standard_deviation'：任何大于X个标准差的差异都会被分割
# 'interquartile'：使用四分位距离来分割块

# 选择使用哪种嵌入方式、断点类型和阈值
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), 
    breakpoint_threshold_type='percentile', 
    breakpoint_threshold_amount=90
)

# 创建文档块
docs = text_splitter.create_documents([content])

# 创建嵌入和向量存储
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(docs, embeddings)
chunks_query_retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 测试查询
test_query = "气候变化的主要原因是什么？"
context = retrieve_context_per_question(test_query, chunks_query_retriever)
show_context(context)
```

## 扩展阅读

- [文本分块最佳实践](https://diamantai.substack.com/p/semantic-chunking-improving-ai-information?r=336pe4&utm_campaign=post&utm_medium=web&triedRedirect=true) 
- [语义分块完整代码](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/semantic_chunking.py)
- [langchain 语义分块](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/)
