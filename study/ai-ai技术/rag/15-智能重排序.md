# 高级检索方法 - 智能重排序 （此方法改变了游戏规则，rag的行业现状，重大突破）

一句话总结： **通过使用交叉编码器模型对文档进行重排序，以提高检索结果的准确性和相关性**

- 专门为相关性排名设计的专用模型

## 实现步骤

### **处理步骤**

#### **1. 向量检索阶段**  

- 初始化一个向量存储（`vectorstore`），用于快速检索与查询相关的候选文档：  

     ```python
     initial_docs = self.vectorstore.similarity_search(query, k=self.k)
     ```

  - `k` 决定了初始检索的文档数量（如 10 个）。  
  - 这一步利用嵌入（embeddings）快速找到可能相关的文档，但结果仅基于向量空间的相似性，精确性有限。

#### **2. 文档-查询对生成**  

- 将检索到的文档与查询组合成对，用于交叉编码器的处理：  

     ```python
     pairs = [[query, doc.page_content] for doc in initial_docs]
     ```

  - 每个文档都会与查询组成一个 `[query, document]` 对，作为交叉编码器的输入。

#### **3. 交叉编码器评分**  

- 使用交叉编码器模型为每个文档-查询对生成相关性评分：  

     ```python
     scores = self.cross_encoder.predict(pairs)
     ```

  - 交叉编码器会同时考虑查询与文档的语义关系，给出更精确的相关性评分。  
  - 使用的模型如 `cross-encoder/ms-marco-MiniLM-L-6-v2` 是专为排序任务设计的。

#### **4. 文档排序**  

- 根据相关性评分对初始检索的文档重新排序：  

     ```python
     scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
     ```

  - 将评分与对应的文档关联，并按照分数从高到低排序。

#### **5. 返回顶级文档**  

- 提取排序后的前 `rerank_top_k` 个文档：  

     ```python
     return [doc for doc, _ in scored_docs[:self.rerank_top_k]]
     ```

  - 返回最相关的文档供后续任务使用。


### **总结步骤**

1. **初步检索**：  
   - 使用向量检索方法快速筛选出可能相关的文档。  

2. **生成文档-查询对**：  
   - 将查询与每个初始检索的文档组合为输入对。  

3. **计算相关性评分**：  
   - 使用交叉编码器对每个对进行精确评分，生成相关性得分。

4. **重新排序**：  
   - 根据交叉编码器的评分对文档排序，获取更高质量的结果。

5. **返回顶级文档**：  
   - 提取排名靠前的文档用于进一步处理（如回答生成、信息抽取等）。  



```py
import os
import sys
from dotenv import load_dotenv
from langchain.docstore.document import Document
from typing import List, Any
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_core.retrievers import BaseRetriever
from sentence_transformers import CrossEncoder
from pydantic import BaseModel, Field
import argparse

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))
from helper_functions import *
from evaluation.evalute_rag import *

# 加载环境变量
load_dotenv()

# 设置 OpenAI API 密钥
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')


# 辅助类和函数

class RatingScore(BaseModel):
    relevance_score: float = Field(..., description="文档与查询的相关性评分。")


def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:
    prompt_template = PromptTemplate(
        input_variables=["query", "doc"],
        template="""请为以下文档与查询的相关性打分，范围为 1-10。请考虑查询的具体上下文和意图，而不仅仅基于关键词匹配。
        查询: {query}
        文档: {doc}
        相关性评分:"""
    )

    llm = ChatOpenAI(temperature=0, model_name="gpt-4o", max_tokens=4000)
    llm_chain = prompt_template | llm.with_structured_output(RatingScore)

    scored_docs = []
    for doc in docs:
        input_data = {"query": query, "doc": doc.page_content}
        score = llm_chain.invoke(input_data).relevance_score
        try:
            score = float(score)
        except ValueError:
            score = 0  # 如果解析失败，默认为 0
        scored_docs.append((doc, score))

    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in reranked_docs[:top_n]]


class CustomRetriever(BaseRetriever, BaseModel):
    vectorstore: Any = Field(description="用于初始检索的向量存储")

    class Config:
        arbitrary_types_allowed = True

    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:
        initial_docs = self.vectorstore.similarity_search(query, k=30)
        return rerank_documents(query, initial_docs, top_n=num_docs)


class CrossEncoderRetriever(BaseRetriever, BaseModel):
    vectorstore: Any = Field(description="用于初始检索的向量存储")
    cross_encoder: Any = Field(description="用于重新排序的交叉编码器模型")
    k: int = Field(default=5, description="初始检索的文档数量")
    rerank_top_k: int = Field(default=3, description="重新排序后返回的文档数量")

    class Config:
        arbitrary_types_allowed = True

    def get_relevant_documents(self, query: str) -> List[Document]:
        initial_docs = self.vectorstore.similarity_search(query, k=self.k)
        pairs = [[query, doc.page_content] for doc in initial_docs]
        scores = self.cross_encoder.predict(pairs)
        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]

    async def aget_relevant_documents(self, query: str) -> List[Document]:
        raise NotImplementedError("异步检索尚未实现")


def compare_rag_techniques(query: str, docs: List[Document]) -> None:
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)

    print("检索技术对比")
    print("==================================")
    print(f"查询: {query}\n")

    print("基础检索结果:")
    baseline_docs = vectorstore.similarity_search(query, k=2)
    for i, doc in enumerate(baseline_docs):
        print(f"\n文档 {i + 1}:")
        print(doc.page_content)

    print("\n高级检索结果:")
    custom_retriever = CustomRetriever(vectorstore=vectorstore)
    advanced_docs = custom_retriever.get_relevant_documents(query)
    for i, doc in enumerate(advanced_docs):
        print(f"\n文档 {i + 1}:")
        print(doc.page_content)


# 主类
class RAGPipeline:
    def __init__(self, path: str):
        self.vectorstore = encode_pdf(path)
        self.llm = ChatOpenAI(temperature=0, model_name="gpt-4o")

    def run(self, query: str, retriever_type: str = "reranker"):
        if retriever_type == "reranker":
            retriever = CustomRetriever(vectorstore=self.vectorstore)
        elif retriever_type == "cross_encoder":
            cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            retriever = CrossEncoderRetriever(
                vectorstore=self.vectorstore,
                cross_encoder=cross_encoder,
                k=10,
                rerank_top_k=5
            )
        else:
            raise ValueError("未知的检索器类型。请使用 'reranker' 或 'cross_encoder'。")

        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )

        result = qa_chain({"query": query})

        print(f"\n问题: {query}")
        print(f"答案: {result['result']}")
        print("\n相关的源文档:")
        for i, doc in enumerate(result["source_documents"]):
            print(f"\n文档 {i + 1}:")
            print(doc.page_content[:200] + "...")


# 参数解析
def parse_args():
    parser = argparse.ArgumentParser(description="RAG 管道")
    parser.add_argument("--path", type=str, default="../data/Understanding_Climate_Change.pdf", help="文档路径")
    parser.add_argument("--query", type=str, default='What are the impacts of climate change?', help="要查询的问题")
    parser.add_argument("--retriever_type", type=str, default="reranker", choices=["reranker", "cross_encoder"],
                        help="要使用的检索器类型")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    pipeline = RAGPipeline(path=args.path)
    pipeline.run(query=args.query, retriever_type=args.retriever_type)

    # 演示重排序的对比
    chunks = [
        "法国的首都是很棒的。",
        "法国的首都是巨大的。",
        "法国的首都是美丽的。",
        """你去过巴黎吗？这是一个美丽的城市，可以品尝美食，看到埃菲尔铁塔。
        我很喜欢法国的所有城市，但拥有埃菲尔铁塔的首都是我最喜欢的城市。""",
        "我很喜欢巴黎之行。这座城市很美，食物很好吃。我很想再去一次。这真是一个了不起的首都城市。"
    ]
    docs = [Document(page_content=sentence) for sentence in chunks]

    compare_rag_techniques(query="法国的首都是哪里？", docs=docs)

```