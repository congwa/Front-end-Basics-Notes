# 分层索引

一句话总结：生成顶层摘要和详细块，然后分别进行向量化索引，自上而下检索。

结论：分层索引通过智能化组织和访问信息的方式，大幅提升了 RAG 系统的性能。它让 AI 系统能更好地把握上下文语境、应对复杂场景，从而提供更精准和更有价值的答案。

```py
import asyncio
import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains.summarize.chain import load_summarize_chain
from langchain.docstore.document import Document
from helper_functions import encode_pdf, encode_from_string

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # 将父目录添加到路径
from helper_functions import *
from evaluation.evalute_rag import *

# 加载环境变量
load_dotenv()

# 设置 OpenAI API 密钥
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')


# 函数：对PDF进行分层编码，生成顶级摘要、中层概述和详细块
async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):
    """
    异步对PDF进行分层编码，包括顶级摘要、中层概述和详细块的向量存储。
    包括使用指数回退处理速率限制。
    """
    # 第1步：加载文档
    # 根据输入类型选择加载方式 - PDF文件或字符串
    if not is_string:
        loader = PyPDFLoader(path)
        documents = await asyncio.to_thread(loader.load)
    else:
        # 使用递归字符分割器进行文本分块
        # chunk_size: 每个块的最大字符数
        # chunk_overlap: 相邻块之间的重叠字符数,避免上下文丢失
        # length_function: 用于计算文本长度的函数
        # is_separator_regex: 是否使用正则表达式作为分隔符
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, is_separator_regex=False
        )
        # 将文本字符串转换为Document对象列表
        documents = text_splitter.create_documents([path])

    # 第2步：建立层次结构
    # — 生成顶级摘要
    # 初始化LLM模型和摘要链
    summary_llm = ChatOpenAI(temperature=0, model_name="gpt-4o-mini", max_tokens=4000)
    summary_chain = load_summarize_chain(summary_llm, chain_type="map_reduce")

    # 定义异步函数处理单个文档的摘要生成
    async def summarize_doc(doc):
        """
        异步处理单个文档生成摘要的函数
        
        参数:
            doc: Document对象,包含待总结的文档内容和元数据
            
        返回:
            Document对象,包含生成的摘要内容和更新后的元数据
        """
        # 使用指数回退重试机制调用摘要链
        # retry_with_exponential_backoff 函数会在失败时自动重试,每次重试间隔时间呈指数增长
        # summary_chain.ainvoke 异步调用摘要生成链,传入文档列表
        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))
        
        # 从输出中提取摘要文本
        summary = summary_output['output_text']
        
        # 返回新的Document对象,包含:
        # - page_content: 生成的摘要文本
        # - metadata: 包含源文件路径、页码信息,以及标记这是一个摘要文档
        return Document(page_content=summary, metadata={
            "source": path,  # 原始文档路径
            "page": doc.metadata["page"],  # 保持原始页码
            "summary": True  # 标记这是摘要文档
        })

    # 批量并行处理文档生成顶层摘要
    summaries = []
    batch_size = 5  # 每批处理5个文档
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        # 并行处理当前批次的文档
        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])
        summaries.extend(batch_summaries)
        await asyncio.sleep(1)  # 添加延迟避免速率限制

    # — 创建中层概述：将原始文档分割成更小的块
    # 创建文本分割器,用于生成中层概述
    # chunk_size: 每个块的最大字符数
    # chunk_overlap: 相邻块之间的重叠字符数,避免上下文丢失
    # length_function: 用于计算文本长度的函数
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)
    
    # 使用分割器将文档分割成更小的块
    # 异步执行以避免阻塞
    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)

    # 为每个详细块添加元数据
    for i, chunk in enumerate(detailed_chunks):
        chunk.metadata.update({"chunk_id": i, "summary": False, "page": int(chunk.metadata.get("page", 0))})

    # 第3步：向量化索引
    # 初始化OpenAI嵌入模型
    embeddings = OpenAIEmbeddings()

    # 定义创建向量存储的异步函数
    async def create_vectorstore(docs):
        return await retry_with_exponential_backoff(asyncio.to_thread(FAISS.from_documents, docs, embeddings))

    # 并行创建摘要和详细块的向量存储
    summary_vectorstore, detailed_vectorstore = await asyncio.gather(
        # summaries 是文档的顶层摘要列表,每个摘要对应一个文档页面的高层次总结
        # detailed_chunks 是文档的详细块列表,包含了文档的完整内容,按chunk_size大小分割
        # summaries 和 detailed_chunks 形成层次化的索引结构:
        # - summaries 作为顶层索引,用于快速定位相关页面
        # - detailed_chunks 作为底层索引,用于获取具体内容
        create_vectorstore(summaries),
        create_vectorstore(detailed_chunks)
    )

    return summary_vectorstore, detailed_vectorstore


# 函数：执行分层检索
def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):
    """
    根据查询执行分层检索。
    """
    # 第4步：检索策略
    # — 实施自上而下的搜索方法
    # 首先在摘要层级进行检索,获取最相关的k_summaries个摘要文档
    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)
    relevant_chunks = []
    
    # 遍历每个相关摘要
    for summary in top_summaries:
        # 获取摘要对应的页码
        page_number = summary.metadata["page"]
        # 定义过滤器函数,只检索指定页码的详细文档
        page_filter = lambda metadata: metadata["page"] == page_number

        # 在详细层级进行检索,获取该页面最相关的k_chunks个文档块
        page_chunks = detailed_vectorstore.similarity_search(query, k=k_chunks, filter=page_filter)
        # 将检索到的文档块添加到结果列表中
        relevant_chunks.extend(page_chunks)
    
    # 返回所有相关的详细文档块
    return relevant_chunks


# 分层检索生成工具类
class HierarchicalRAG:
    def __init__(self, pdf_path, chunk_size=1000, chunk_overlap=200):
        self.pdf_path = pdf_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.summary_store = None
        self.detailed_store = None

    async def run(self, query):
        # 检查是否存在已保存的向量存储
        if os.path.exists("../vector_stores/summary_store") and os.path.exists("../vector_stores/detailed_store"):
            # 如果存在,加载已有的向量存储
            embeddings = OpenAIEmbeddings()
            self.summary_store = FAISS.load_local("../vector_stores/summary_store", embeddings, allow_dangerous_deserialization=True)
            self.detailed_store = FAISS.load_local("../vector_stores/detailed_store", embeddings, allow_dangerous_deserialization=True)
        else:
            # 如果不存在,重新构建向量存储
            self.summary_store, self.detailed_store = await encode_pdf_hierarchical(self.pdf_path, self.chunk_size, self.chunk_overlap)
            # 保存向量存储到本地
            self.summary_store.save_local("../vector_stores/summary_store")
            self.detailed_store.save_local("../vector_stores/detailed_store")

        # 执行分层检索
        results = retrieve_hierarchical(query, self.summary_store, self.detailed_store)
        # 打印检索结果
        for chunk in results:
            print(f"页码: {chunk.metadata['page']}")
            print(f"内容: {chunk.page_content}...")
            print("---")


# 参数解析器
def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description="在指定PDF上运行分层RAG。")
    parser.add_argument("--pdf_path", type=str, default="../data/Understanding_Climate_Change.pdf", help="PDF文档的路径。")
    parser.add_argument("--chunk_size", type=int, default=1000, help="每个文本块的大小。")
    parser.add_argument("--chunk_overlap", type=int, default=200, help="连续块之间的重叠。")
    parser.add_argument("--query", type=str, default='温室效应是什么',
                        help="要在文档中搜索的查询。")
    return parser.parse_args()


# 主程序
if __name__ == "__main__":
    args = parse_args()
    rag = HierarchicalRAG(args.pdf_path, args.chunk_size, args.chunk_overlap)
    asyncio.run(rag.run(args.query))


```

   ## 参考资料

   - [hierarchical-indices-enhancing-rag](https://diamantai.substack.com/p/hierarchical-indices-enhancing-rag?r=336pe4&utm_campaign=post&utm_medium=web&triedRedirect=true)