# 分块 - 选择分块大小

## 概述 🔎

在RAG (检索增强生成) 系统中，选择合适的文本分块大小是一个关键决策。这个选择需要在以下两个方面之间取得平衡：

- **上下文保留**: 较大的块大小可以保持更多上下文信息
- **检索效率**: 较小的块大小可以提高检索精度和速度

## 实现细节 🛠️

本实现提供了一个完整的评估框架，用于测试不同分块大小对RAG系统性能的影响。主要评估指标包括：

1. 响应时间
2. 忠实度 (Faithfulness)
3. 相关性 (Relevancy)

### 核心功能

- 支持多种分块大小的实验对比
- 使用GPT-3.5-turbo进行查询处理
- 使用GPT-4进行评估
- 包含完整的评估指标计算

### 代码实现

```python
import nest_asyncio
import random
import time
import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.prompts import PromptTemplate
from llama_index.core.evaluation import DatasetGenerator, FaithfulnessEvaluator, RelevancyEvaluator
from llama_index.llms.openai import OpenAI

# 应用asyncio修复（用于Jupyter notebooks）
nest_asyncio.apply()

# 加载环境变量
load_dotenv()

# 设置OpenAI API密钥
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

def evaluate_response_time_and_accuracy(chunk_size, eval_questions, eval_documents, faithfulness_evaluator,
                                     relevancy_evaluator):
    """
    评估特定分块大小下GPT-3.5-turbo的平均响应时间、忠实度和相关性。

    参数:
    chunk_size (int): 数据分块的大小
    eval_questions (list): 评估问题列表
    eval_documents (list): 用于评估的文档
    faithfulness_evaluator (FaithfulnessEvaluator): 忠实度评估器
    relevancy_evaluator (RelevancyEvaluator): 相关性评估器

    返回:
    tuple: 包含平均响应时间、忠实度和相关性指标的元组
    """
    total_response_time = 0
    total_faithfulness = 0
    total_relevancy = 0

    # 设置全局LLM为GPT-3.5
    llm = OpenAI(model="gpt-3.5-turbo")
    Settings.llm = llm
    
    # 创建向量索引
    vector_index = VectorStoreIndex.from_documents(eval_documents)

    # 构建查询引擎
    query_engine = vector_index.as_query_engine(similarity_top_k=5)
    num_questions = len(eval_questions)

    # 遍历每个评估问题计算指标
    for question in eval_questions:
        start_time = time.time()
        response_vector = query_engine.query(question)
        elapsed_time = time.time() - start_time

        faithfulness_result = faithfulness_evaluator.evaluate_response(response=response_vector).passing
        relevancy_result = relevancy_evaluator.evaluate_response(query=question, response=response_vector).passing

        total_response_time += elapsed_time
        total_faithfulness += faithfulness_result
        total_relevancy += relevancy_result

    average_response_time = total_response_time / num_questions
    average_faithfulness = total_faithfulness / num_questions
    average_relevancy = total_relevancy / num_questions

    return average_response_time, average_faithfulness, average_relevancy

class RAGEvaluator:
    """
    RAG系统评估器
    用于评估不同分块大小对系统性能的影响
    """
    def __init__(self, data_dir, num_eval_questions, chunk_sizes):
        self.data_dir = data_dir
        self.num_eval_questions = num_eval_questions
        self.chunk_sizes = chunk_sizes
        self.documents = self.load_documents()
        self.eval_questions = self.generate_eval_questions()
        # 设置GPT-4作为本地评估配置
        self.llm_gpt4 = OpenAI(model="gpt-4")
        self.faithfulness_evaluator = self.create_faithfulness_evaluator()
        self.relevancy_evaluator = self.create_relevancy_evaluator()

    def load_documents(self):
        """加载文档"""
        return SimpleDirectoryReader(self.data_dir).load_data()

    def generate_eval_questions(self):
        """生成评估问题"""
        eval_documents = self.documents[0:20]
        data_generator = DatasetGenerator.from_documents(eval_documents)
        eval_questions = data_generator.generate_questions_from_nodes()
        return random.sample(eval_questions, self.num_eval_questions)

    def create_faithfulness_evaluator(self):
        """创建忠实度评估器"""
        faithfulness_evaluator = FaithfulnessEvaluator(llm=self.llm_gpt4)
        faithfulness_new_prompt_template = PromptTemplate("""
            请判断给定的信息是否直接被上下文支持。
            你需要回答YES或NO。
            如果上下文的任何部分明确支持该信息，即使大部分上下文不相关，也请回答YES。
            如果上下文没有明确支持该信息，请回答NO。
            以下是一些示例：
            ...
            """)
        faithfulness_evaluator.update_prompts({"your_prompt_key": faithfulness_new_prompt_template})
        return faithfulness_evaluator

    def create_relevancy_evaluator(self):
        """创建相关性评估器"""
        return RelevancyEvaluator(llm=self.llm_gpt4)

    def run(self):
        """运行评估"""
        for chunk_size in self.chunk_sizes:
            avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(
                chunk_size,
                self.eval_questions,
                self.documents[0:20],
                self.faithfulness_evaluator,
                self.relevancy_evaluator
            )
            print(f"分块大小 {chunk_size} - 平均响应时间: {avg_response_time:.2f}秒, "
                  f"平均忠实度: {avg_faithfulness:.2f}, 平均相关性: {avg_relevancy:.2f}")

def parse_args():
    """解析命令行参数"""
    import argparse
    parser = argparse.ArgumentParser(description='RAG方法评估')
    parser.add_argument('--data_dir', type=str, default='../data', help='文档目录')
    parser.add_argument('--num_eval_questions', type=int, default=25, help='评估问题数量')
    parser.add_argument('--chunk_sizes', nargs='+', type=int, default=[128, 256], help='分块大小列表')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    evaluator = RAGEvaluator(
        data_dir=args.data_dir,
        num_eval_questions=args.num_eval_questions,
        chunk_sizes=args.chunk_sizes
    )
    evaluator.run()
```

## 使用指南 📖

1. **环境设置**
   - 确保已安装所有必要的依赖
   - 配置OpenAI API密钥

2. **运行评估**
   ```bash
   python rag_evaluator.py --data_dir ../data --num_eval_questions 25 --chunk_sizes 128 256
   ```

3. **参数说明**
   - `data_dir`: 文档所在目录
   - `num_eval_questions`: 生成的评估问题数量
   - `chunk_sizes`: 要测试的分块大小列表

## 最佳实践 💡

1. **选择分块大小的考虑因素**:
   - 文档的性质和结构
   - 查询的典型长度和复杂度
   - 系统的性能要求
   - 可用的计算资源

2. **建议的分块大小范围**:
   - 文本文档: 128-512 tokens
   - 代码文档: 256-1024 tokens
   - 技术文档: 256-768 tokens

3. **性能优化建议**:
   - 对于需要快速响应的应用，考虑使用较小的分块大小
   - 对于需要深入理解的应用，考虑使用较大的分块大小
   - 可以通过实验找到特定用例的最佳平衡点
