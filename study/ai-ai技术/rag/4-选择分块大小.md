# åˆ†å— - é€‰æ‹©åˆ†å—å¤§å°

## æ¦‚è¿° ğŸ”

åœ¨RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) ç³»ç»Ÿä¸­ï¼Œé€‰æ‹©åˆé€‚çš„æ–‡æœ¬åˆ†å—å¤§å°æ˜¯ä¸€ä¸ªå…³é”®å†³ç­–ã€‚è¿™ä¸ªé€‰æ‹©éœ€è¦åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢ä¹‹é—´å–å¾—å¹³è¡¡ï¼š

- **ä¸Šä¸‹æ–‡ä¿ç•™**: è¾ƒå¤§çš„å—å¤§å°å¯ä»¥ä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
- **æ£€ç´¢æ•ˆç‡**: è¾ƒå°çš„å—å¤§å°å¯ä»¥æé«˜æ£€ç´¢ç²¾åº¦å’Œé€Ÿåº¦

## å®ç°ç»†èŠ‚ ğŸ› ï¸

æœ¬å®ç°æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•ä¸åŒåˆ†å—å¤§å°å¯¹RAGç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚ä¸»è¦è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š

1. å“åº”æ—¶é—´
2. å¿ å®åº¦ (Faithfulness)
3. ç›¸å…³æ€§ (Relevancy)

### æ ¸å¿ƒåŠŸèƒ½

- æ”¯æŒå¤šç§åˆ†å—å¤§å°çš„å®éªŒå¯¹æ¯”
- ä½¿ç”¨GPT-3.5-turboè¿›è¡ŒæŸ¥è¯¢å¤„ç†
- ä½¿ç”¨GPT-4è¿›è¡Œè¯„ä¼°
- åŒ…å«å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—

### ä»£ç å®ç°

```python
import nest_asyncio
import random
import time
import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.prompts import PromptTemplate
from llama_index.core.evaluation import DatasetGenerator, FaithfulnessEvaluator, RelevancyEvaluator
from llama_index.llms.openai import OpenAI

# åº”ç”¨asyncioä¿®å¤ï¼ˆç”¨äºJupyter notebooksï¼‰
nest_asyncio.apply()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®OpenAI APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

def evaluate_response_time_and_accuracy(chunk_size, eval_questions, eval_documents, faithfulness_evaluator,
                                     relevancy_evaluator):
    """
    è¯„ä¼°ç‰¹å®šåˆ†å—å¤§å°ä¸‹GPT-3.5-turboçš„å¹³å‡å“åº”æ—¶é—´ã€å¿ å®åº¦å’Œç›¸å…³æ€§ã€‚

    å‚æ•°:
    chunk_size (int): æ•°æ®åˆ†å—çš„å¤§å°
    eval_questions (list): è¯„ä¼°é—®é¢˜åˆ—è¡¨
    eval_documents (list): ç”¨äºè¯„ä¼°çš„æ–‡æ¡£
    faithfulness_evaluator (FaithfulnessEvaluator): å¿ å®åº¦è¯„ä¼°å™¨
    relevancy_evaluator (RelevancyEvaluator): ç›¸å…³æ€§è¯„ä¼°å™¨

    è¿”å›:
    tuple: åŒ…å«å¹³å‡å“åº”æ—¶é—´ã€å¿ å®åº¦å’Œç›¸å…³æ€§æŒ‡æ ‡çš„å…ƒç»„
    """
    total_response_time = 0
    total_faithfulness = 0
    total_relevancy = 0

    # è®¾ç½®å…¨å±€LLMä¸ºGPT-3.5
    llm = OpenAI(model="gpt-3.5-turbo")
    Settings.llm = llm
    
    # åˆ›å»ºå‘é‡ç´¢å¼•
    vector_index = VectorStoreIndex.from_documents(eval_documents)

    # æ„å»ºæŸ¥è¯¢å¼•æ“
    query_engine = vector_index.as_query_engine(similarity_top_k=5)
    num_questions = len(eval_questions)

    # éå†æ¯ä¸ªè¯„ä¼°é—®é¢˜è®¡ç®—æŒ‡æ ‡
    for question in eval_questions:
        start_time = time.time()
        response_vector = query_engine.query(question)
        elapsed_time = time.time() - start_time

        faithfulness_result = faithfulness_evaluator.evaluate_response(response=response_vector).passing
        relevancy_result = relevancy_evaluator.evaluate_response(query=question, response=response_vector).passing

        total_response_time += elapsed_time
        total_faithfulness += faithfulness_result
        total_relevancy += relevancy_result

    average_response_time = total_response_time / num_questions
    average_faithfulness = total_faithfulness / num_questions
    average_relevancy = total_relevancy / num_questions

    return average_response_time, average_faithfulness, average_relevancy

class RAGEvaluator:
    """
    RAGç³»ç»Ÿè¯„ä¼°å™¨
    ç”¨äºè¯„ä¼°ä¸åŒåˆ†å—å¤§å°å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“
    """
    def __init__(self, data_dir, num_eval_questions, chunk_sizes):
        self.data_dir = data_dir
        self.num_eval_questions = num_eval_questions
        self.chunk_sizes = chunk_sizes
        self.documents = self.load_documents()
        self.eval_questions = self.generate_eval_questions()
        # è®¾ç½®GPT-4ä½œä¸ºæœ¬åœ°è¯„ä¼°é…ç½®
        self.llm_gpt4 = OpenAI(model="gpt-4")
        self.faithfulness_evaluator = self.create_faithfulness_evaluator()
        self.relevancy_evaluator = self.create_relevancy_evaluator()

    def load_documents(self):
        """åŠ è½½æ–‡æ¡£"""
        return SimpleDirectoryReader(self.data_dir).load_data()

    def generate_eval_questions(self):
        """ç”Ÿæˆè¯„ä¼°é—®é¢˜"""
        eval_documents = self.documents[0:20]
        data_generator = DatasetGenerator.from_documents(eval_documents)
        eval_questions = data_generator.generate_questions_from_nodes()
        return random.sample(eval_questions, self.num_eval_questions)

    def create_faithfulness_evaluator(self):
        """åˆ›å»ºå¿ å®åº¦è¯„ä¼°å™¨"""
        faithfulness_evaluator = FaithfulnessEvaluator(llm=self.llm_gpt4)
        faithfulness_new_prompt_template = PromptTemplate("""
            è¯·åˆ¤æ–­ç»™å®šçš„ä¿¡æ¯æ˜¯å¦ç›´æ¥è¢«ä¸Šä¸‹æ–‡æ”¯æŒã€‚
            ä½ éœ€è¦å›ç­”YESæˆ–NOã€‚
            å¦‚æœä¸Šä¸‹æ–‡çš„ä»»ä½•éƒ¨åˆ†æ˜ç¡®æ”¯æŒè¯¥ä¿¡æ¯ï¼Œå³ä½¿å¤§éƒ¨åˆ†ä¸Šä¸‹æ–‡ä¸ç›¸å…³ï¼Œä¹Ÿè¯·å›ç­”YESã€‚
            å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰æ˜ç¡®æ”¯æŒè¯¥ä¿¡æ¯ï¼Œè¯·å›ç­”NOã€‚
            ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š
            ...
            """)
        faithfulness_evaluator.update_prompts({"your_prompt_key": faithfulness_new_prompt_template})
        return faithfulness_evaluator

    def create_relevancy_evaluator(self):
        """åˆ›å»ºç›¸å…³æ€§è¯„ä¼°å™¨"""
        return RelevancyEvaluator(llm=self.llm_gpt4)

    def run(self):
        """è¿è¡Œè¯„ä¼°"""
        for chunk_size in self.chunk_sizes:
            avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(
                chunk_size,
                self.eval_questions,
                self.documents[0:20],
                self.faithfulness_evaluator,
                self.relevancy_evaluator
            )
            print(f"åˆ†å—å¤§å° {chunk_size} - å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’, "
                  f"å¹³å‡å¿ å®åº¦: {avg_faithfulness:.2f}, å¹³å‡ç›¸å…³æ€§: {avg_relevancy:.2f}")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='RAGæ–¹æ³•è¯„ä¼°')
    parser.add_argument('--data_dir', type=str, default='../data', help='æ–‡æ¡£ç›®å½•')
    parser.add_argument('--num_eval_questions', type=int, default=25, help='è¯„ä¼°é—®é¢˜æ•°é‡')
    parser.add_argument('--chunk_sizes', nargs='+', type=int, default=[128, 256], help='åˆ†å—å¤§å°åˆ—è¡¨')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    evaluator = RAGEvaluator(
        data_dir=args.data_dir,
        num_eval_questions=args.num_eval_questions,
        chunk_sizes=args.chunk_sizes
    )
    evaluator.run()
```

## ä½¿ç”¨æŒ‡å— ğŸ“–

1. **ç¯å¢ƒè®¾ç½®**
   - ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–
   - é…ç½®OpenAI APIå¯†é’¥

2. **è¿è¡Œè¯„ä¼°**
   ```bash
   python rag_evaluator.py --data_dir ../data --num_eval_questions 25 --chunk_sizes 128 256
   ```

3. **å‚æ•°è¯´æ˜**
   - `data_dir`: æ–‡æ¡£æ‰€åœ¨ç›®å½•
   - `num_eval_questions`: ç”Ÿæˆçš„è¯„ä¼°é—®é¢˜æ•°é‡
   - `chunk_sizes`: è¦æµ‹è¯•çš„åˆ†å—å¤§å°åˆ—è¡¨

## æœ€ä½³å®è·µ ğŸ’¡

1. **é€‰æ‹©åˆ†å—å¤§å°çš„è€ƒè™‘å› ç´ **:
   - æ–‡æ¡£çš„æ€§è´¨å’Œç»“æ„
   - æŸ¥è¯¢çš„å…¸å‹é•¿åº¦å’Œå¤æ‚åº¦
   - ç³»ç»Ÿçš„æ€§èƒ½è¦æ±‚
   - å¯ç”¨çš„è®¡ç®—èµ„æº

2. **å»ºè®®çš„åˆ†å—å¤§å°èŒƒå›´**:
   - æ–‡æœ¬æ–‡æ¡£: 128-512 tokens
   - ä»£ç æ–‡æ¡£: 256-1024 tokens
   - æŠ€æœ¯æ–‡æ¡£: 256-768 tokens

3. **æ€§èƒ½ä¼˜åŒ–å»ºè®®**:
   - å¯¹äºéœ€è¦å¿«é€Ÿå“åº”çš„åº”ç”¨ï¼Œè€ƒè™‘ä½¿ç”¨è¾ƒå°çš„åˆ†å—å¤§å°
   - å¯¹äºéœ€è¦æ·±å…¥ç†è§£çš„åº”ç”¨ï¼Œè€ƒè™‘ä½¿ç”¨è¾ƒå¤§çš„åˆ†å—å¤§å°
   - å¯ä»¥é€šè¿‡å®éªŒæ‰¾åˆ°ç‰¹å®šç”¨ä¾‹çš„æœ€ä½³å¹³è¡¡ç‚¹
