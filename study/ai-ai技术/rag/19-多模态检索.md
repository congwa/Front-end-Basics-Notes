# 多模态检索

一句话总结： 图片和文本和多种类型混合嵌入、检索，输出图片和文本等多种类型

现在常见大模型都支持多模态嵌入了

```python
import os
import openai
import faiss
import numpy as np
from PIL import Image
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import AnalyzeDocumentChain
from langchain.chains.summarize import load_summarize_chain
from langchain.llms import OpenAI

# 通过OpenAI API进行图像处理和文本生成
openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. 图像编码函数：将图像转化为特征向量
def encode_image(image_path):
    """
    该函数将图像转换为OpenAI的嵌入向量（此处使用假数据）。
    """
    image = Image.open(image_path)
    # 这里你可以使用 CLIP 等模型生成图像的向量
    # 这里只是一个简化示例，实际情况需要使用合适的深度学习模型来获取图像嵌入
    image_embedding = np.random.rand(1, 512)  # 假数据：512维的随机向量
    return image_embedding

# 2. 文本编码函数：将文本转换为嵌入向量
def encode_text(text):
    embeddings = OpenAIEmbeddings()
    return embeddings.embed_query(text)

# 3. 索引文本和图像
def create_index(texts, images):
    # 创建FAISS索引
    embeddings = OpenAIEmbeddings()

    # 将文本和图像编码为嵌入向量
    text_embeddings = [encode_text(text) for text in texts]
    image_embeddings = [encode_image(image) for image in images]

    # 合并文本和图像的嵌入
    all_embeddings = text_embeddings + image_embeddings
    all_embeddings = np.vstack(all_embeddings)

    # 创建FAISS索引
    index = faiss.IndexFlatL2(all_embeddings.shape[1])
    index.add(all_embeddings)
    return index

# 4. 检索相似的文本或图像
def retrieve_from_index(query, index, texts, images, k=5):
    query_embedding = encode_text(query)  # 将查询转换为嵌入向量
    query_embedding = np.array(query_embedding).reshape(1, -1)

    # 在FAISS索引中进行搜索
    D, I = index.search(query_embedding, k)

    # 根据索引返回检索到的文本和图像
    retrieved_texts = [texts[i] for i in I[0][:len(texts)]]
    retrieved_images = [images[i - len(texts)] for i in I[0][len(texts):]]
    return retrieved_texts, retrieved_images

# 5. 生成总结与回答
def generate_answer(query, retrieved_texts, retrieved_images):
    # 将检索到的文本和图像拼接在一起作为生成模型的输入
    combined_input = "\n".join(retrieved_texts) + "\n" + "\n".join([f"Image: {img}" for img in retrieved_images])

    # 使用OpenAI的语言模型生成回答
    response = openai.Completion.create(
        model="text-davinci-003",  # 使用 GPT-3 或 GPT-4
        prompt=f"基于以下信息，回答问题: \n{combined_input}\n问题: {query}",
        max_tokens=200
    )

    return response.choices[0].text.strip()

# 6. 将所有步骤整合在一起
def multimodal_rag(query, texts, images):
    # 创建索引
    index = create_index(texts, images)

    # 检索相关文本和图像
    retrieved_texts, retrieved_images = retrieve_from_index(query, index, texts, images)

    # 生成回答
    answer = generate_answer(query, retrieved_texts, retrieved_images)
    return answer

# 示例：文本和图像
texts = ["The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.", "Paris is the capital city of France."]
images = ["image1.jpg", "image2.jpg"]

# 用户查询
query = "Where is the Eiffel Tower?"

# 运行多模态检索
answer = multimodal_rag(query, texts, images)
print("Answer:", answer)

```
