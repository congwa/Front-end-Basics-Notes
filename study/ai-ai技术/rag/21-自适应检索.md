# 自适应检索

一句话总结： 考虑用户上下文和偏好，将查询分为不同类别，并为每个类别使用定制的检索策略。

生成多种查询策略，利用llm命中某个策略，进行查询优化


```py
# 一个基于langchain的例子
import os
import sys
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate

from langchain_core.retrievers import BaseRetriever
from typing import List, Dict, Any
from langchain.docstore.document import Document
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field

sys.path.append(os.path.abspath(
    os.path.join(os.getcwd(), '..')))  # 将父目录添加到路径中，方便使用 notebook 文件夹中的模块
from helper_functions import *
from evaluation.evalute_rag import *

# 从 .env 文件加载环境变量
load_dotenv()

# 设置 OpenAI API 密钥的环境变量
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')


# 定义所有必要的类和策略
class CategoriesOptions(BaseModel):
    category: str = Field(
        description="查询的类别。选项为：事实性 (Factual)、分析性 (Analytical)、意见性 (Opinion) 或上下文相关 (Contextual)",
        example="事实性"
    )


class RelevantScore(BaseModel):
    score: float = Field(description="文档对查询的相关性评分", example=8.0)


class SelectedIndices(BaseModel):
    indices: List[int] = Field(description="选定文档的索引", example=[0, 1, 2, 3])


class SubQueries(BaseModel):
    sub_queries: List[str] = Field(description="用于全面分析的子查询列表",
                                   example=["纽约市的人口是多少？", "纽约市的 GDP 是多少？"])


class QueryClassifier:
    def __init__(self):
        self.llm = ChatOpenAI(temperature=0, model_name="gpt-4o", max_tokens=4000)
        self.prompt = PromptTemplate(
            input_variables=["query"],
            template="将以下查询分类为以下四类之一：事实性、分析性、意见性或上下文相关。\n查询：{query}\n类别："
        )
        self.chain = self.prompt | self.llm.with_structured_output(CategoriesOptions)

    def classify(self, query):
        print("正在分类查询...")
        return self.chain.invoke(query).category


class BaseRetrievalStrategy:
    def __init__(self, texts):
        self.embeddings = OpenAIEmbeddings()
        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)
        self.documents = text_splitter.create_documents(texts)
        self.db = FAISS.from_documents(self.documents, self.embeddings)
        self.llm = ChatOpenAI(temperature=0, model_name="gpt-4o", max_tokens=4000)

    def retrieve(self, query, k=4):
        return self.db.similarity_search(query, k=k)


class FactualRetrievalStrategy(BaseRetrievalStrategy):
    def retrieve(self, query, k=4):
        print("正在检索事实性信息...")
        enhanced_query_prompt = PromptTemplate(
            input_variables=["query"],
            template="优化以下事实性查询以提高信息检索效果：{query}"
        )
        query_chain = enhanced_query_prompt | self.llm
        enhanced_query = query_chain.invoke(query).content
        print(f'优化后的查询：{enhanced_query}')

        docs = self.db.similarity_search(enhanced_query, k=k * 2)

        ranking_prompt = PromptTemplate(
            input_variables=["query", "doc"],
            template="按照 1-10 的评分标准，评估此文档与查询 '{query}' 的相关性。\n文档：{doc}\n相关性评分："
        )
        ranking_chain = ranking_prompt | self.llm.with_structured_output(RelevantScore)

        ranked_docs = []
        print("正在排序文档...")
        for doc in docs:
            input_data = {"query": enhanced_query, "doc": doc.page_content}
            score = float(ranking_chain.invoke(input_data).score)
            ranked_docs.append((doc, score))

        ranked_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked_docs[:k]]


class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):
    def retrieve(self, query, k=4):
        print("正在检索分析性信息...")
        sub_queries_prompt = PromptTemplate(
            input_variables=["query", "k"],
            template="为以下查询生成 {k} 个子问题：{query}"
        )
        sub_queries_chain = sub_queries_prompt | self.llm.with_structured_output(SubQueries)
        input_data = {"query": query, "k": k}
        sub_queries = sub_queries_chain.invoke(input_data).sub_queries
        print(f'生成的子问题：{sub_queries}')

        all_docs = []
        for sub_query in sub_queries:
            all_docs.extend(self.db.similarity_search(sub_query, k=2))

        diversity_prompt = PromptTemplate(
            input_variables=["query", "docs", "k"],
            template="为查询 '{query}' 选择最相关且多样化的 {k} 篇文档：\n文档：{docs}\n"
        )
        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)
        docs_text = "\n".join([f"{i}: {doc.page_content[:50]}..." for i, doc in enumerate(all_docs)])
        input_data = {"query": query, "docs": docs_text, "k": k}
        selected_indices = diversity_chain.invoke(input_data).indices

        return [all_docs[i] for i in selected_indices if i < len(all_docs)]


class OpinionRetrievalStrategy(BaseRetrievalStrategy):
    def retrieve(self, query, k=3):
        print("正在检索意见信息...")
        viewpoints_prompt = PromptTemplate(
            input_variables=["query", "k"],
            template="为主题 {query} 确定 {k} 个不同的观点或角度。"
        )
        viewpoints_chain = viewpoints_prompt | self.llm
        input_data = {"query": query, "k": k}
        viewpoints = viewpoints_chain.invoke(input_data).content.split('\n')
        print(f'生成的观点：{viewpoints}')

        all_docs = []
        for viewpoint in viewpoints:
            all_docs.extend(self.db.similarity_search(f"{query} {viewpoint}", k=2))

        opinion_prompt = PromptTemplate(
            input_variables=["query", "docs", "k"],
            template="将以下文档分类为与查询 '{query}' 相关的不同意见，并选择最具代表性的 {k} 个观点：\n文档：{docs}\n选择的索引："
        )
        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)

        docs_text = "\n".join([f"{i}: {doc.page_content[:100]}..." for i, doc in enumerate(all_docs)])
        input_data = {"query": query, "docs": docs_text, "k": k}
        selected_indices = opinion_chain.invoke(input_data).indices

        return [all_docs[int(i)] for i in selected_indices if i.isdigit() and int(i) < len(all_docs)]


class ContextualRetrievalStrategy(BaseRetrievalStrategy):
    def retrieve(self, query, k=4, user_context=None):
        print("正在检索上下文相关的信息...")
        context_prompt = PromptTemplate(
            input_variables=["query", "context"],
            template="根据用户的上下文：{context}\n重新表述查询，以便更好地满足用户需求：{query}"
        )
        context_chain = context_prompt | self.llm
        input_data = {"query": query, "context": user_context or "未提供具体上下文"}
        contextualized_query = context_chain.invoke(input_data).content
        print(f'上下文优化后的查询：{contextualized_query}')

        docs = self.db.similarity_search(contextualized_query, k=k * 2)

        ranking_prompt = PromptTemplate(
            input_variables=["query", "context", "doc"],
            template="根据查询：'{query}' 和用户上下文：'{context}'，对以下文档的相关性进行评分（1-10）：\n文档：{doc}\n相关性评分："
        )
        ranking_chain = ranking_prompt | self.llm.with_structured_output(RelevantScore)

        ranked_docs = []
        for doc in docs:
            input_data = {"query": contextualized_query, "context": user_context or "未提供具体上下文",
                          "doc": doc.page_content}
            score = float(ranking_chain.invoke(input_data).score)
            ranked_docs.append((doc, score))

        ranked_docs.sort(key=lambda x: x[1], reverse=True)

        return [doc for doc, _ in ranked_docs[:k]]


# 定义主类 Adaptive RAG
class AdaptiveRAG:
    def __init__(self, texts: List[str]):
        self.classifier = QueryClassifier()
        self.strategies = {
            "Factual": FactualRetrievalStrategy(texts),
            "Analytical": AnalyticalRetrievalStrategy(texts),
            "Opinion": OpinionRetrievalStrategy(texts),
            "Contextual": ContextualRetrievalStrategy(texts)
        }
        self.llm = ChatOpenAI(temperature=0, model_name="gpt-4o", max_tokens=4000)
        prompt_template = """使用以下内容作为上下文回答问题。
        如果无法回答，请直接说“我不知道”，不要尝试编造答案。

        {context}

        问题：{question}
        答案："""
        self.prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        self.llm_chain = self.prompt | self.llm

    def answer(self, query: str) -> str:
        category = self.classifier.classify(query)
        strategy = self.strategies[category]
        docs = strategy.retrieve(query)
        input_data = {"context": "\n".join([doc.page_content for doc in docs]), "question": query}
        return self.llm_chain.invoke(input_data).content


# 解析命令行参数
def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description="运行 AdaptiveRAG 系统。")
    parser.add_argument('--texts', nargs='+', help="用于检索的输入文本")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    texts = args.texts or [
        "地球是距离太阳第三近的行星，也是已知唯一能够孕育生命的天体。"]
    rag_system = AdaptiveRAG(texts)

    queries = [
        "地球与太阳之间的距离是多少？",
        "地球与太阳的距离如何影响气候？",
        "关于地球生命起源的不同理论有哪些？",
        "地球在太阳系中的位置如何影响其宜居性？"
    ]

    for query in queries:
        print(f"查询：{query}")
        result = rag_system.answer(query)
        print(f"答案：{result}")

```


## 例子

- [自适应检索 langgraph](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag.ipynb)
