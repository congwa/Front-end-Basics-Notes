# 分块 - 命题分块

## 动机

命题分块方法背后的动机是构建一个系统，将文本文档分解为简洁、事实的命题，以实现更细粒度和更精确的信息检索。使用命题可以更好地控制和处理特定查询，特别是从详细或复杂的文本中提取知识。使用较小的命题块和较大的文档块之间的比较旨在评估粒度信息检索的有效性。

## 方法详情

1. 加载环境变量
- 加载 LLM 服务的 API 密钥等环境变量
- 确保系统可访问必要资源

2. 文本处理
- 使用 RecursiveCharacterTextSplitter 进行文档分块
- 使用 llama-3.1-70b-versatile 生成命题
- 生成独立的事实陈述列表
- 通过第二个 LLM 进行质量评估
  - 评估准确性
  - 评估清晰度  
  - 评估完整性
  - 评估简洁性

3. 向量化与检索
- 使用 OllamaEmbeddings 模型将命题嵌入向量存储
- 构建两个检索系统:
  - 基于命题的块检索
  - 基于大文档块的检索
- 通过多个查询测试比较性能

## 优点

1. 粒度
通过将文档分解为小的事实命题，系统可以进行高度具体的检索，从而更容易从大型或复杂的文档中提取精确的答案。

2. 质量保证
使用质量检查 LLM 可确保生成的命题符合特定标准，从而提高检索信息的可靠性。

3. 检索的灵活性
基于命题的检索和基于较大块的检索之间的比较允许评估搜索结果中粒度和更广泛上下文之间的权衡。

## 实现

### 1. 命题生成
LLM 与自定义提示结合使用，从文档块中生成事实陈述。

### 2. 质量检查
生成的命题通过评分系统，评估准确性、清晰度、完整性和简洁性。

### 3. 向量存储集成
命题在使用预先训练的嵌入模型嵌入后存储在 FAISS 向量存储中，从而实现基于相似性的高效搜索和检索。

### 4. 查询测试
对向量存储（基于命题和更大的块）进行多个测试查询以比较检索性能。


```py
### LLMs
import os
from dotenv import load_dotenv

# 从'.env'文件加载环境变量
load_dotenv()

os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY') # 用于LLM
sample_content = """Paul Graham's essay "Founder Mode," published in September 2024, challenges conventional wisdom about scaling startups, arguing that founders should maintain their unique management style rather than adopting traditional corporate practices as their companies grow.
Conventional Wisdom vs. Founder Mode
The essay argues that the traditional advice given to growing companies—hiring good people and giving them autonomy—often fails when applied to startups.
This approach, suitable for established companies, can be detrimental to startups where the founder's vision and direct involvement are crucial. "Founder Mode" is presented as an emerging paradigm that is not yet fully understood or documented, contrasting with the conventional "manager mode" often advised by business schools and professional managers.
Unique Founder Abilities
Founders possess unique insights and abilities that professional managers do not, primarily because they have a deep understanding of their company's vision and culture.
Graham suggests that founders should leverage these strengths rather than conform to traditional managerial practices. "Founder Mode" is an emerging paradigm that is not yet fully understood or documented, with Graham hoping that over time, it will become as well-understood as the traditional manager mode, allowing founders to maintain their unique approach even as their companies scale.
Challenges of Scaling Startups
As startups grow, there is a common belief that they must transition to a more structured managerial approach. However, many founders have found this transition problematic, as it often leads to a loss of the innovative and agile spirit that drove the startup's initial success.
Brian Chesky, co-founder of Airbnb, shared his experience of being advised to run the company in a traditional managerial style, which led to poor outcomes. He eventually found success by adopting a different approach, influenced by how Steve Jobs managed Apple.
Steve Jobs' Management Style
Steve Jobs' management approach at Apple served as inspiration for Brian Chesky's "Founder Mode" at Airbnb. One notable practice was Jobs' annual retreat for the 100 most important people at Apple, regardless of their position on the organizational chart
. This unconventional method allowed Jobs to maintain a startup-like environment even as Apple grew, fostering innovation and direct communication across hierarchical levels. Such practices emphasize the importance of founders staying deeply involved in their companies' operations, challenging the traditional notion of delegating responsibilities to professional managers as companies scale.
"""

### 构建索引
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings

# 设置嵌入模型
embedding_model = OllamaEmbeddings(model='nomic-embed-text:v1.5', show_progress=True)

# 文档
docs_list = [Document(page_content=sample_content, metadata={"Title": "Paul Graham's Founder Mode Essay", "Source": "https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ"})]

# 分块
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=200, chunk_overlap=50
)

doc_splits = text_splitter.split_documents(docs_list)

for i, doc in enumerate(doc_splits):
    doc.metadata['chunk_id'] = i+1 ### 添加块ID

from typing import List
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_groq import ChatGroq

# 数据模型
class GeneratePropositions(BaseModel):
    """文档中所有命题的列表"""

    propositions: List[str] = Field(
        description="命题列表(事实性、自包含且简洁的信息)"
    )


# 带函数调用的LLM
llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0)
structured_llm= llm.with_structured_output(GeneratePropositions)

# 分解成一系列的命题
proposition_examples = [
    {"document": 
        "In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.", 
     "propositions": 
        "['Neil Armstrong was an astronaut.', 'Neil Armstrong walked on the Moon in 1969.', 'Neil Armstrong was the first person to walk on the Moon.', 'Neil Armstrong walked on the Moon during the Apollo 11 mission.', 'The Apollo 11 mission occurred in 1969.']"
    },
]

example_proposition_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{document}"),
        ("ai", "{propositions}"),
    ]
)

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt = example_proposition_prompt,
    examples = proposition_examples,
)

# 提示词
system = """请将以下文本分解为简单的、自包含的命题。确保每个命题满足以下标准：

    1. 表达单一事实：每个命题应陈述一个具体的事实或主张。
    2. 无需上下文即可理解：命题应该是自包含的，无需额外上下文即可理解。
    3. 使用全名而非代词：避免使用代词或模糊引用；使用完整的实体名称。
    4. 包含相关日期/限定词：如果适用，包含必要的日期、时间和限定词以使事实准确。
    5. 包含一个主谓关系：专注于单一主语及其相应的动作或属性，避免使用连词或多个从句。"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        few_shot_prompt,
        ("human", "{document}"),
    ]
)

proposition_generator = prompt | structured_llm
propositions = [] # 存储文档中的所有命题

for i in range(len(doc_splits)):
    response = proposition_generator.invoke({"document": doc_splits[i].page_content}) # 创建命题
    for proposition in response.propositions:
        propositions.append(Document(page_content=proposition, metadata={"Title": "Paul Graham's Founder Mode Essay", "Source": "https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ", "chunk_id": i+1}))

# 数据模型
class GradePropositions(BaseModel):
    """根据准确性、清晰度、完整性和简洁性对给定命题进行评分"""

    accuracy: int = Field(
        description="基于命题对原文的反映程度评分1-10"
    )
    
    clarity: int = Field(
        description="基于命题在无需额外上下文的情况下的可理解程度评分1-10"
    )

    completeness: int = Field(
        description="基于命题是否包含必要细节(如日期、限定词)评分1-10"
    )

    conciseness: int = Field(
        description="基于命题是否在不失去重要信息的情况下保持简洁评分1-10"
    )

# 带函数调用的LLM
llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0)
structured_llm= llm.with_structured_output(GradePropositions)

# 提示词
evaluation_prompt_template = """
请根据以下标准评估命题：
- **准确性**：基于命题对原文的反映程度评分1-10。
- **清晰度**：基于命题在无需额外上下文的情况下的可理解程度评分1-10。
- **完整性**：基于命题是否包含必要细节(如日期、限定词)评分1-10。
- **简洁性**：基于命题是否在不失去重要信息的情况下保持简洁评分1-10。

示例：
文档：In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.

命题1：Neil Armstrong was an astronaut.
评分1："accuracy": 10, "clarity": 10, "completeness": 10, "conciseness": 10

命题2：Neil Armstrong walked on the Moon in 1969.
评分2："accuracy": 10, "clarity": 10, "completeness": 10, "conciseness": 10

命题3：Neil Armstrong was the first person to walk on the Moon.
评分3："accuracy": 10, "clarity": 10, "completeness": 10, "conciseness": 10

命题4：Neil Armstrong walked on the Moon during the Apollo 11 mission.
评分4："accuracy": 10, "clarity": 10, "completeness": 10, "conciseness": 10

命题5：The Apollo 11 mission occurred in 1969.
评分5："accuracy": 10, "clarity": 10, "completeness": 10, "conciseness": 10

格式：
命题："{proposition}"
原文："{original_text}"
"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", evaluation_prompt_template),
        ("human", "{proposition}, {original_text}"),
    ]
)

proposition_evaluator = prompt | structured_llm
# 定义评估类别和阈值
evaluation_categories = ["accuracy", "clarity", "completeness", "conciseness"]
thresholds = {"accuracy": 7, "clarity": 7, "completeness": 7, "conciseness": 7}

# 评估命题的函数
def evaluate_proposition(proposition, original_text):
    response = proposition_evaluator.invoke({"proposition": proposition, "original_text": original_text})
    
    # 解析响应以提取分数
    scores = {"accuracy": response.accuracy, "clarity": response.clarity, "completeness": response.completeness, "conciseness": response.conciseness}
    return scores

# 检查命题是否通过质量检查
def passes_quality_check(scores):
    for category, score in scores.items():
        if score < thresholds[category]:
            return False
    return True

evaluated_propositions = [] # 存储文档中的所有命题

# 遍历生成的命题并评估它们
for idx, proposition in enumerate(propositions):
    scores = evaluate_proposition(proposition.page_content, doc_splits[proposition.metadata['chunk_id'] - 1].page_content)
    if passes_quality_check(scores):
        # 命题通过质量检查，保留它
        evaluated_propositions.append(proposition)
    else:
        # 命题未通过，丢弃或标记以供进一步审查
        print(f"{idx+1}) 命题：{proposition.page_content} \n 分数：{scores}")
        print("未通过")

# 添加到向量存储
vectorstore_propositions = FAISS.from_documents(evaluated_propositions, embedding_model)
retriever_propositions = vectorstore_propositions.as_retriever(
                search_type="similarity",
                search_kwargs={'k': 4}, # 要检索的文档数量
            )

query = "谁的管理方法为Airbnb的Brian Chesky的\"Founder Mode\"提供了灵感？"
res_proposition = retriever_propositions.invoke(query)

for i, r in enumerate(res_proposition):
    print(f"{i+1}) 内容：{r.page_content} --- 块ID：{r.metadata['chunk_id']}")

# 添加到向量存储_larger
vectorstore_larger = FAISS.from_documents(doc_splits, embedding_model)
retriever_larger = vectorstore_larger.as_retriever(
                search_type="similarity",
                search_kwargs={'k': 4}, # 要检索的文档数量
```

## 结论


| 方面 | 基于命题的检索 | 简单的块检索 |
|------|----------------|--------------|
| 响应精确 | 高：提供集中且直接的答案 | 中：提供更多上下文，但可能包含不相关的信息 |
| 清晰简洁 | 高：清晰简洁，避免不必要的细节 | 中等：更全面，但可能会让人不知所措 |
| 情境丰富度 | 低：可能缺乏背景，专注于具体的主张 | 高：提供额外的背景和细节 |
| 综合性 | 低：可能会省略更广泛的背景或补充细节 | 高：提供更完整的视图和广泛的信息 |
| 叙事流程 | 中等：可能是支离破碎或脱节的 | 高：保留原始文档的逻辑流程和连贯性 |
| 信息过载 | 低：不太可能被过多的信息淹没 | 高：过多信息可能会导致用户不知所措 |
| 用例适用性 | 最适合快速、真实的查询 | 最适合需要深入理解的复杂查询 |
| 效率 | 高：提供快速、有针对性的响应 | 中：可能需要更多努力来筛选其他内容 |
| 特异性 | 高：精确且有针对性的响应 | 中：由于包含了更广泛的背景，答案可能不太有针对性 |

## 扩展阅读

- [命题方法：增强人工智能系统的信息检索](https://diamantai.substack.com/p/the-propositions-method-enhancing?r=336pe4&utm_campaign=post&utm_medium=web&triedRedirect=true)
