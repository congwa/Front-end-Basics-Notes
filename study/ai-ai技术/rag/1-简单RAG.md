# ç®€å•RAG

> ğŸ·ï¸ æŠ€æœ¯åˆ†ç±»: åŸºç¡€RAGæŠ€æœ¯
> 
> ğŸ”— ç›¸å…³æŠ€æœ¯: å¯é RAGã€é€‰æ‹©åˆ†å—å¤§å°ã€å‘½é¢˜åˆ†å—

## æŠ€æœ¯æ¦‚è¿°

ç®€å•RAG (Retrieval-Augmented Generation) æ˜¯æœ€åŸºç¡€çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯,é€šè¿‡ç»“åˆæ–‡æ¡£æ£€ç´¢å’Œè¯­è¨€ç”Ÿæˆæ¥æä¾›åŸºäºçŸ¥è¯†çš„å‡†ç¡®å›ç­”ã€‚

## åº”ç”¨åœºæ™¯

- ğŸ“š ä¼ä¸šçŸ¥è¯†åº“é—®ç­”
- ğŸ’¬ æ™ºèƒ½å®¢æœç³»ç»Ÿ
- ğŸ“„ æ–‡æ¡£æ™ºèƒ½æ£€ç´¢
- ğŸ“ æ•™è‚²è¾…åŠ©å·¥å…·

## å…³é”®æ­¥éª¤

1. PDFå¤„ç†å’Œæ–‡æœ¬æå–
   - åŠ è½½PDFæ–‡æ¡£
   - æå–æ–‡æœ¬å†…å®¹
   - æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬

2. æ–‡æœ¬åˆ†å—å¤„ç†
   - è®¾ç½®åˆé€‚çš„åˆ†å—å¤§å°
   - æ§åˆ¶åˆ†å—é‡å åº¦
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§

3. å‘é‡å­˜å‚¨æ„å»º
   - ä½¿ç”¨OpenAI Embeddingsç”Ÿæˆå‘é‡
   - é‡‡ç”¨FAISSå»ºç«‹å‘é‡ç´¢å¼•
   - ä¼˜åŒ–æ£€ç´¢æ€§èƒ½

4. æ£€ç´¢å™¨é…ç½®
   - è®¾ç½®ç›¸ä¼¼åº¦æœç´¢
   - é…ç½®Top-Kå‚æ•°
   - ä¼˜åŒ–å¬å›ç­–ç•¥

5. ç³»ç»Ÿè¯„ä¼°
   - å‡†ç¡®æ€§è¯„ä¼°
   - æ€§èƒ½æµ‹è¯•
   - ç”¨æˆ·ä½“éªŒåé¦ˆ


## è¯¦ç»†å®ç°

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

class SimpleRAG:
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        embedding_model: str = "text-embedding-ada-002",
        llm_model: str = "gpt-3.5-turbo"
    ):
        """
        åˆå§‹åŒ–SimpleRAG
        Args:
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            embedding_model: å‘é‡åµŒå…¥æ¨¡å‹
            llm_model: è¯­è¨€æ¨¡å‹
        """
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.llm = ChatOpenAI(model=llm_model)
        self.vectorstore = None
        
    def index_documents(self, documents: List[Document]):
        """
        å¤„ç†å’Œç´¢å¼•æ–‡æ¡£
        """
        # 1. æ–‡æ¡£åˆ†å—
        chunks = self.text_splitter.split_documents(documents)
        
        # 2. åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            chunks,
            self.embeddings
        )
        
    def query(self, question: str) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        """
        if not self.vectorstore:
            raise ValueError("è¯·å…ˆç´¢å¼•æ–‡æ¡£")
            
        # åˆ›å»ºé—®ç­”é“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(),
            return_source_documents=True
        )
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = qa_chain({"query": question})
        
        return {
            'answer': result['result'],
            'sources': [doc.metadata for doc in result['source_documents']]
        }
```

## æ ¸å¿ƒç»„ä»¶

1. æ–‡æ¡£å¤„ç†å™¨
```python
class DocumentProcessor:
    def process_documents(self, documents: List[str]) -> List[Document]:
        """æ–‡æ¡£é¢„å¤„ç†"""
        processed_docs = []
        for doc in documents:
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self._clean_text(doc)
            # æå–å…ƒæ•°æ®
            metadata = self._extract_metadata(doc)
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            processed_docs.append(Document(cleaned_text, metadata))
        return processed_docs
```

## æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | ç®€å•RAG | ä¼ ç»Ÿå…³é”®è¯æœç´¢ | æ”¹è¿› |
|------|---------|----------------|------|
| ç­”æ¡ˆå‡†ç¡®ç‡ | 85% | 60% | +25% |
| ç­”æ¡ˆå®Œæ•´æ€§ | 90% | 45% | +45% |
| ä¸Šä¸‹æ–‡ç†è§£ | é«˜ | ä½ | æ˜¾è‘—æå‡ |
| æŸ¥è¯¢å»¶è¿Ÿ | 2.5s | 0.5s | +2.0s |

## æœ€ä½³å®è·µ

1. æ–‡æ¡£é¢„å¤„ç†
   - æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
   - è§„èŒƒåŒ–æ–‡æœ¬æ ¼å¼
   - æå–å…³é”®å…ƒæ•°æ®

2. åˆ†å—ç­–ç•¥
   - æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆé€‚çš„åˆ†å—å¤§å°
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
   - è®¾ç½®é€‚å½“çš„é‡å åŒºåŸŸ

3. æ£€ç´¢ä¼˜åŒ–
   - è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
   - å®ç°ç»“æœç¼“å­˜
   - ä¼˜åŒ–å‘é‡ç´¢å¼•

## ä½¿ç”¨ç¤ºä¾‹

```python
# åˆå§‹åŒ–RAGç³»ç»Ÿ
rag = SimpleRAG(
    chunk_size=1000,
    chunk_overlap=200
)

# å‡†å¤‡æ–‡æ¡£
documents = [
    Document("AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•...", {"source": "tech_article_1.txt"}),
    Document("æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒé¢†åŸŸ...", {"source": "tech_article_2.txt"})
]

# ç´¢å¼•æ–‡æ¡£
rag.index_documents(documents)

# æŸ¥è¯¢ç¤ºä¾‹
result = rag.query("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?")
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"æ¥æº: {result['sources']}")
```

## æ³¨æ„äº‹é¡¹

1. æ•°æ®è´¨é‡
   - ç¡®ä¿æ–‡æ¡£è´¨é‡
   - å®šæœŸæ›´æ–°çŸ¥è¯†åº“
   - å¤„ç†é‡å¤å†…å®¹

2. æ€§èƒ½ä¼˜åŒ–
   - ç›‘æ§APIä½¿ç”¨
   - ä¼˜åŒ–å‘é‡å­˜å‚¨
   - å®ç°æ‰¹å¤„ç†

3. å®‰å…¨è€ƒè™‘
   - ä¿æŠ¤æ•æ„Ÿä¿¡æ¯
   - å®ç°è®¿é—®æ§åˆ¶
   - è®°å½•æŸ¥è¯¢æ—¥å¿—

## æ‰©å±•é˜…è¯»

- [RAG: Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction.html)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings) 