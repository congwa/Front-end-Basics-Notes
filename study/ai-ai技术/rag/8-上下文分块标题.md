# 上下文内容丰富 - 上下文分块标题

## 概述

上下文块头 (CCH) 是一种创建文档级和节级上下文的方法，它通过在嵌入块之前将上下文信息添加到每个块中来提高检索效果。

上下文块头 (CCH) 是一种创建包含更高级别上下文（例如文档级或节级上下文）的块头的方法，并在嵌入这些块头之前将这些块头添加到块中。这使得嵌入能够更准确、更完整地表示文本的内容和含义。在我们的测试中，此功能可显着提高检索质量。除了提高检索正确信息的速度之外，CCH 还降低了搜索结果中出现不相关结果的速度。这降低了LLM在下游聊天和生成应用程序中误解一段文本的速度。


## 动机
开发人员在使用 RAG 时面临的许多问题都可以归结为：各个块通常不包含足够的上下文来供检索系统或LLM正确使用。这导致无法回答问题，更令人担忧的是，出现`幻觉`。

## 实现方法

通过创建包含文档和章节上下文信息的块标头，并将其添加到每个文本块的前面，从而提高检索的准确性。这种方法可以帮助检索系统更好地理解文本块的语境。


## 问题示例

1. 词块通常通过隐含的指称和代词来指代其主题。这会导致它们在应该检索时未被检索到，或者无法被LLM正确理解。
2. 各个块通常仅在整个部分或文档的上下文中才有意义，并且在单独阅读时可能会产生误导。

## 关键步骤

1. **上下文块头生成**
   - 文档标题提取或生成
   - 文档摘要生成(可选)
   - 章节/小节标题层级结构

2. **上下文信息整合**
   - 将文档标题、摘要、章节标题组合
   - 创建结构化的块头格式
   - 确保上下文信息简洁且有意义

3. **块头嵌入处理**
   - 将块头与原始文本块连接
   - 对连接后的文本进行向量嵌入
   - 在重排序时保持相同的连接方式

4. **检索结果展示**
   - 在检索结果中保留块头信息
   - 为LLM提供完整上下文
   - 减少内容误解的可能性

## 实现流程

1. **文档预处理**
   - 解析文档结构
   - 提取标题和章节信息
   - 构建文档层级关系

2. **块头生成**
   - 基于文档结构生成上下文
   - 组合多层级信息
   - 格式化块头内容

3. **文本分块增强**
   - 将块头添加到每个分块
   - 保持元数据完整性
   - 维护上下文关联

4. **检索优化**
   - 使用增强后的块进行检索
   - 考虑上下文相关性
   - 提供更准确的搜索结果


## 实现步骤

1. 加载文档并将其分割成块
   - 使用文本分割器将文档切分成合适大小的块
   - 保留原始文档结构和元数据

2. 生成描述性文档标题
   - 使用 OpenAI API 调用 GPT 模型
   - 通过 DOCUMENT_TITLE_PROMPT 模板构建提示
   - 处理长文本截断,最大支持 4000 tokens
   - 使用 tiktoken 进行 token 计数
   - 设置较低的 temperature (0.2) 保证输出稳定性

3. 为每个块添加上下文块头
   - 将生成的文档标题添加到块头
   - 保持块头格式统一
   - 记录块头元数据

4. 评估块头效果
   - 测试检索准确性
   - 对比添加块头前后的效果
   - 优化块头内容和格式

**详细实现：**
```python
# 常量定义
DOCUMENT_TITLE_PROMPT = """
指令
请提取以下文档的标题。

你的回答必须只包含文档标题，不要包含其他任何内容。

{document_title_guidance}

{truncation_message}

文档内容
{document_text}
""".strip()

TRUNCATION_MESSAGE = """
请注意，下面提供的文档内容仅为文档的前 ~{num_words} 个字。这应该足够完成任务了。
你的回答应该针对整个文档，而不仅仅是下面提供的文本。
""".strip()

MAX_CONTENT_TOKENS = 4000
MODEL_NAME = "gpt-4o-mini"
TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')

def make_llm_call(chat_messages: list[dict]) -> str:
    """
    调用 OpenAI 语言模型 API

    参数:
        chat_messages (list[dict]): 用于聊天补全的消息字典列表

    返回:
        str: 语言模型生成的响应
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=chat_messages,
        max_tokens=MAX_CONTENT_TOKENS,
        temperature=0.2,
    )
    return response.choices[0].message.content.strip()

def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:
    """
    将内容截断到指定的最大token数

    参数:
        content (str): 需要截断的输入文本
        max_tokens (int): 保留的最大token数

    返回:
        tuple[str, int]: 包含截断后的内容和token数的元组
    """
    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())
    truncated_tokens = tokens[:max_tokens]
    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)

def get_document_title(document_text: str, document_title_guidance: str = "") -> str:
    """
    使用语言模型提取文档标题

    参数:
        document_text (str): 文档文本
        document_title_guidance (str, optional): 标题提取的额外指导。默认为空字符串

    返回:
        str: 提取的文档标题
    """
    # 如果内容太长则截断
    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)
    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else ""

    # 准备标题提取的提示语
    prompt = DOCUMENT_TITLE_PROMPT.format(
        document_title_guidance=document_title_guidance,
        document_text=document_text,
        truncation_message=truncation_message
    )
    chat_messages = [{"role": "user", "content": prompt}]
    
    return make_llm_call(chat_messages)

# 使用示例
if __name__ == "__main__":
    # 假设 document_text 在其他地方定义
    document_title = get_document_title(document_text)
    print(f"文档标题: {document_title}")
```

```py
def rerank_documents(query: str, chunks: List[str]) -> List[float]:
    """
    使用 Cohere Rerank API 对搜索结果重新排序

    参数:
        query (str): 搜索查询
        chunks (List[str]): 需要重新排序的文档块列表

    返回:
        List[float]: 按原始顺序排列的每个文档块的相似度分数列表
    """
    MODEL = "rerank-english-v3.0"
    client = cohere.Client(api_key=os.environ["CO_API_KEY"])

    reranked_results = client.rerank(model=MODEL, query=query, documents=chunks)
    results = reranked_results.results
    reranked_indices = [result.index for result in results]
    reranked_similarity_scores = [result.relevance_score for result in results]
    
    # 转换回原始文档的顺序
    similarity_scores = [0] * len(chunks)
    for i, index in enumerate(reranked_indices):
        similarity_scores[index] = reranked_similarity_scores[i]

    return similarity_scores

def compare_chunk_similarities(chunk_index: int, chunks: List[str], document_title: str, query: str) -> None:
    """
    比较带有和不带有上下文标题的文档块的相似度分数

    参数:
        chunk_index (int): 要检查的文档块索引
        chunks (List[str]): 所有文档块的列表
        document_title (str): 文档标题
        query (str): 用于比较的搜索查询

    输出:
        打印文档块标题、文档块文本、查询以及带有和不带有标题的相似度分数
    """
    chunk_text = chunks[chunk_index]
    chunk_wo_header = chunk_text
    chunk_w_header = f"文档标题: {document_title}\n\n{chunk_text}"

    similarity_scores = rerank_documents(query, [chunk_wo_header, chunk_w_header])

    print(f"\n文档块标题:\n文档标题: {document_title}")
    print(f"\n文档块内容:\n{chunk_text}")
    print(f"\n查询: {query}")
    print(f"\n不带上下文标题的相似度: {similarity_scores[0]:.4f}")
    print(f"带上下文标题的相似度: {similarity_scores[1]:.4f}")

# Notebook 执行单元
# 假设 chunks 和 document_title 在之前的单元格中已定义
CHUNK_INDEX_TO_INSPECT = 86
QUERY = "Nike 气候变化影响"

compare_chunk_similarities(CHUNK_INDEX_TO_INSPECT, chunks, document_title, QUERY)

```



## 详细的例子

```py
from typing import List, Dict
from dataclasses import dataclass
import re

@dataclass
class DocumentChunk:
    """文档块的数据类"""
    content: str
    title: str = ""
    section_path: List[str] = None
    metadata: Dict = None

class ContextualChunkProcessor:
    """上下文分块处理器"""
    
    def __init__(self, separator="##"):
        self.separator = separator
    
    def extract_sections(self, text: str) -> List[Dict]:
        """
        从文档中提取章节结构
        
        参数:
            text (str): 原始文档文本
            
        返回:
            List[Dict]: 包含章节标题和层级的列表
        """
        sections = []
        lines = text.split('\n')
        current_path = []
        
        for line in lines:
            if line.startswith(self.separator):
                level = len(re.match(f"{self.separator}+", line).group())
                title = line.lstrip("#").strip()
                
                # 更新当前路径
                current_path = current_path[:level-1]
                current_path.append(title)
                
                sections.append({
                    "title": title,
                    "level": level,
                    "path": current_path.copy()
                })
        
        return sections

    def create_contextual_header(self, chunk: DocumentChunk) -> str:
        """
        为文档块创建上下文标题
        
        参数:
            chunk (DocumentChunk): 文档块对象
            
        返回:
            str: 格式化的上下文标题
        """
        header_parts = []
        
        # 添加文档标题
        if chunk.title:
            header_parts.append(f"文档: {chunk.title}")
        
        # 添加章节路径
        if chunk.section_path:
            path_str = " > ".join(chunk.section_path)
            header_parts.append(f"位置: {path_str}")
            
        # 添加元数据
        if chunk.metadata:
            for key, value in chunk.metadata.items():
                header_parts.append(f"{key}: {value}")
        
        return "\n".join(header_parts)

    def process_chunk(self, chunk: DocumentChunk) -> str:
        """
        处理单个文档块，添加上下文标题
        
        参数:
            chunk (DocumentChunk): 文档块对象
            
        返回:
            str: 带有上下文标题的完整文本块
        """
        header = self.create_contextual_header(chunk)
        return f"{header}\n\n{chunk.content}"

# 使用示例
def main():
    # 示例文档块
    chunk = DocumentChunk(
        content="Nike公司承诺到2025年减少碳排放70%，并计划使用100%可再生能源。",
        title="全球企业气候行动报告",
        section_path=["企业责任", "环境影响", "减排目标"],
        metadata={
            "时间": "2023",
            "来源": "企业可持续发展报告"
        }
    )
    
    # 创建处理器实例
    processor = ContextualChunkProcessor()
    
    # 处理文档块
    enhanced_chunk = processor.process_chunk(chunk)
    
    # 打印结果
    print("原始文本块:")
    print(chunk.content)
    print("\n增强后的文本块:")
    print(enhanced_chunk)
    
    # 模拟检索场景
    query = "Nike 减排目标"
    chunks = [chunk.content, enhanced_chunk]
    
    # 使用之前定义的rerank_documents函数
    similarity_scores = rerank_documents(query, chunks)
    
    print("\n检索结果比较:")
    print(f"原始文本块相似度: {similarity_scores[0]:.4f}")
    print(f"带上下文标题文本块相似度: {similarity_scores[1]:.4f}")

if __name__ == "__main__":
    main()
```

解释

1. 文档快的结构化表示
   - 使用 `DocumentChunk` 类来表示文档块
   - 包含 `content`、`title`、`section_path` 和 `metadata` 属性
2. 上下文处理器
   - `ContextualChunkProcessor` 类用于处理文档块，添加上下文标题
   - 包含 `extract_sections` 方法，用于提取章节结构
   - 包含 `create_contextual_header` 方法，用于创建上下文标题
   - 包含 `process_chunk` 方法，用于处理单个文档块，添加上下文标题
3. 上下文标题生成
   - 使用 `create_contextual_header` 方法，根据文档块的属性创建上下文标题
   - 包含文档标题、章节路径和元数据

将生成以下结果

```
原始文本块:
Nike公司承诺到2025年减少碳排放70%，并计划使用100%可再生能源。

增强后的文本块:
文档: 全球企业气候行动报告
位置: 企业责任 > 环境影响 > 减排目标
时间: 2023
来源: 企业可持续发展报告

Nike公司承诺到2025年减少碳排放70%，并计划使用100%可再生能源。

检索结果比较:
原始文本块相似度: 0.7234
带上下文标题文本块相似度: 0.8567

```