# 2025-03-05

- [ ] Langchain-Chatchat源码 - 文本分块方式的有哪些? 在本项目中如何使用的？


## 4种文本分割器

1. **AliTextSplitter**: 文档语义分割模型为达摩院开源的nlp_bert_document-segmentation_chinese-base。支持去除pdf ocr提取后的文档多余的换行符和空白字符。
2. **ChineseTextSplitter**: 首先使用标点符号进行初步分句，然后针对超过`sentence_size`属性定义的长度的句子，进行进一步的分割，确保每个句子的长度都不会超过设定的最大值。支持去除pdf ocr提取后的文档多余的换行符和空白字符。
3. **ChineseRecursiveTextSplitter**: 首先根据用户配置的分隔符，然后根据这个分隔符将文本分割成小片段。如果某个片段长度超过预设的阈值，该方法会递归地对该片段进行进一步分割，直到所有片段的长度都符合要求。最终，方法返回一个经过清理（去除多余的换行符和空白字符）的文本片段列表
4. **zh_title_enhance**: 提取一组文档块的标题，把它放到元数据中，方便以后使用。

各有特点，根据实际本文情况进行选择吧

## 项目中如何使用

`zh_title_enhance`如果配置了，就先试用`zh_title_enhance`对文档处理一遍，之后走以下流程

参考 `make_text_splitter`方法

创建知识库的时候指定配置的`self.text_splitter_name = Settings.kb_settings.TEXT_SPLITTER_NAME`分词器名字， 默认`RecursiveCharacterTextSplitter`

优先级顺序：

1. Markdown专用分割器（如果指定）
  
2. 自定义分割器（如果存在）

- LangChain内置分割器
- 后备方案：RecursiveCharacterTextSplitter
  
3. 是否配置了分词器来源：

- tiktoken：适用于OpenAI相关模型,  从tiktoken加载分词器， 需要加载对应配置， 尝试配置 zh_core_web_sm 不支持就不配
- HuggingFace：支持多种预训练模型， 从HuggingFace加载分词器 需要加载对应配置  尝试zh_core_web_sm 不支持就不配
- 默认：基础字符分割

4. 以上都没有，使用简单的分块处理

- 中文支持：通过 zh_core_web_sm ，不支持就简单分块

5. 如果以上的有异常

- 降级策略：确保始终返回可用的分割器 `RecursiveCharacterTextSplitter`


总结： 优先使用用户选择的分析器，如果没有，那么使用默认配置的 `RecursiveCharacterTextSplitter`





